{"cells":[{"cell_type":"markdown","metadata":{"id":"rQsd9PutvxYS"},"source":["# Book 3 - Modelling and Evaluation (DistilBert Model)\n","\n","---\n"]},{"cell_type":"code","execution_count":23,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-08-23T13:38:33.925015Z","iopub.status.busy":"2022-08-23T13:38:33.924497Z","iopub.status.idle":"2022-08-23T13:38:33.929811Z","shell.execute_reply":"2022-08-23T13:38:33.928751Z","shell.execute_reply.started":"2022-08-23T13:38:33.924974Z"},"id":"wMqf8jrybYJf","outputId":"d702a417-14a2-489b-e549-2c1163e782a9","trusted":true},"outputs":[],"source":["# # ! pip install accelerate nvidia-ml-py3\n","# ! pip install datasets==2.1.0\n","# ! pip install transformers==4.18.0\n","# ! pip install sentencepiece==0.1.96\n","# ! pip install pytorch-lightning==1.6.5\n","# ! pip install torchmetrics==0.9.2\n","# ! pip install wandb==0.12.21"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:33.931957Z","iopub.status.busy":"2022-08-23T13:38:33.931542Z","iopub.status.idle":"2022-08-23T13:38:34.869221Z","shell.execute_reply":"2022-08-23T13:38:34.867941Z","shell.execute_reply.started":"2022-08-23T13:38:33.931916Z"},"id":"_Qo3zsKzbYJi","trusted":true},"outputs":[],"source":["import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        os.path.join(dirname, filename)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n","\n","import pandas as pd\n","pd.set_option(\"max_colwidth\", None)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import re\n","from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW, get_cosine_schedule_with_warmup, EarlyStoppingCallback, AutoModel\n","from datasets import Dataset, Value, ClassLabel, Features, load_metric\n","import math\n","\n","import torch\n","from torch import nn\n","from torch.utils.checkpoint import checkpoint # need to call when using gradient_checkpointing\n","\n","from sklearn.metrics import log_loss, confusion_matrix\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import train_test_split\n","\n","from scipy.special import softmax\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import seed_everything\n","from pytorch_lightning.callbacks import EarlyStopping, TQDMProgressBar,StochasticWeightAveraging\n","from torchmetrics.functional import f1_score\n","\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","import spacy\n","from spacy import displacy\n","\n","import wandb\n","\n","from text_unidecode import unidecode\n","from typing import Dict, List, Tuple\n","import codecs"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:34.872060Z","iopub.status.busy":"2022-08-23T13:38:34.871687Z","iopub.status.idle":"2022-08-23T13:38:35.034922Z","shell.execute_reply":"2022-08-23T13:38:35.033435Z","shell.execute_reply.started":"2022-08-23T13:38:34.872028Z"},"id":"W3zN9GhJeqtx","outputId":"437b4dc8-ee1b-4acb-bcbb-f7138887beb4","trusted":true},"outputs":[],"source":["input_location = 1\n","\n","if input_location ==0:\n","  INPUT_DIR = './data/'\n","  \n","elif input_location == 1:\n","  INPUT_DIR = '/kaggle/input/feedback-prize-effectiveness/'\n","  TRAINED_DIR = './'\n","elif input_location ==2:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  INPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/Data/'\n","  \n","data_path = pd.read_csv(INPUT_DIR + 'train.csv')\n","test_path = pd.read_csv(INPUT_DIR + 'test.csv')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bHeiH9uLelCz"},"source":["# Check GPU availability"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:35.037412Z","iopub.status.busy":"2022-08-23T13:38:35.036957Z","iopub.status.idle":"2022-08-23T13:38:35.043487Z","shell.execute_reply":"2022-08-23T13:38:35.042254Z","shell.execute_reply.started":"2022-08-23T13:38:35.037368Z"},"id":"gYfDzL4vbYJi","outputId":"a8f1d7b2-c145-45e4-ffb2-547facccf30a","trusted":true},"outputs":[],"source":["device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"tQi2UWQaelC2"},"source":["# Login to Weight and Bias"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:35.053395Z","iopub.status.busy":"2022-08-23T13:38:35.052962Z","iopub.status.idle":"2022-08-23T13:38:55.092177Z","shell.execute_reply":"2022-08-23T13:38:55.090958Z","shell.execute_reply.started":"2022-08-23T13:38:35.053364Z"},"id":"ZvmYfktWDgpz","outputId":"17c6a9e8-3637-4bed-ef27-cb3949a4dcef","trusted":true},"outputs":[],"source":["try:\n","  wandb_api = open('/content/drive/MyDrive/Colab Notebooks/WandBapi/wandbapi.txt', 'r').read()\n","  !wandb login {wandb_api}\n","except:\n","  try:\n","    from kaggle_secrets import UserSecretsClient\n","    user_secrets = UserSecretsClient()\n","    secret_value = user_secrets.get_secret(\"wand_api\")\n","    !wandb login {secret_value}\n","\n","  except:\n","    print(\"wandb failed to login...\")\n","    \n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.094418Z","iopub.status.busy":"2022-08-23T13:38:55.094100Z","iopub.status.idle":"2022-08-23T13:38:55.099710Z","shell.execute_reply":"2022-08-23T13:38:55.098671Z","shell.execute_reply.started":"2022-08-23T13:38:55.094389Z"},"id":"fNfe3qbFvV5b","outputId":"045b755f-40f3-43cd-b580-b469d3d00655","trusted":true},"outputs":[],"source":["import time\n","timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n","print(f'Current datetime: {timestr}')"]},{"cell_type":"markdown","metadata":{"id":"PAKCk5__elC2"},"source":["# Configuration"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.101718Z","iopub.status.busy":"2022-08-23T13:38:55.101406Z","iopub.status.idle":"2022-08-23T13:38:55.132649Z","shell.execute_reply":"2022-08-23T13:38:55.131368Z","shell.execute_reply.started":"2022-08-23T13:38:55.101688Z"},"id":"q06p8Uj8elC3","outputId":"67bf3f9a-1611-44bd-8577-f2bd8d5bfd94","trusted":true},"outputs":[],"source":["\n","attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n","\n","# try:\n","#   data_path = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\n","#   test_path = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\n","# except:\n","#   data_path = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/train.csv')\n","#   test_path = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/test.csv')\n","\n","distilbert_config={'name': 'distilbert',\n","#                     'model_name':'distilbert-base-uncased',\n","                   'model_name':'../input/distilbertbaseuncased/distilbert-base-uncased',\n","                    'existing_tuned_model_name' :'',# 'kitkeat/distilbert-based-uncased-argumentativewriting',\n","                    'newly_tuned_model_path' : '../input/distilbert-trained-model-20220820/20220820-043647.pth',\n","                    # 'PATH':'/content/drive/MyDrive/Colab Notebooks/FineTuneModel/distilbert-frozenembedding&2transformlayer-5epoch-lr6e5-drop02.pth',\n","                    # 'model_name': '../input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad',\n","                    # 'PATH' : '../input/distilberttuned/distilbert-frozenembedding2transformlayer-5epoch-lr6e5-drop02.pth',\n","                    'wandb':False,\n","                    'param':{\n","                      'n_labels': 3,\n","                      'batch_size': 64,\n","                      'lr': 8e-4,#6e-5,\n","                      'warmup': 0, \n","                      'weight_decay': 0.01,#Default is 0.01\n","                      'n_epochs': 5,#4,\n","                      'n_freeze' : 5,\n","                      'p_dropout':0,#0.2,#0.6,\n","                      'scheduler':False,\n","                      'precision':16, #Default is 32\n","                      'sample_mode':True,\n","                      'sample_size': 100,\n","                        'swa':False,\n","                        'swa_lrs':1e-2\n","                        \n","                  }\n","              }\n","\n","seed_everything(91, workers=True)"]},{"cell_type":"markdown","metadata":{"id":"Ei6ESM-jelC4"},"source":["# Utility"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.134500Z","iopub.status.busy":"2022-08-23T13:38:55.133870Z","iopub.status.idle":"2022-08-23T13:38:55.146298Z","shell.execute_reply":"2022-08-23T13:38:55.145371Z","shell.execute_reply.started":"2022-08-23T13:38:55.134467Z"},"id":"ERcD4UBAelC5","trusted":true},"outputs":[],"source":["# Freeze the hidden layer within the pretrained model\n","def freeze(module):\n","    \"\"\"\n","    Freezes module's parameters.\n","    \"\"\"\n","    \n","    for parameter in module.parameters():\n","        parameter.requires_grad = False\n","        \n","def get_freezed_parameters(module):\n","    \"\"\"\n","    Returns names of freezed parameters of the given module.\n","    \"\"\"\n","    \n","    freezed_parameters = []\n","    for name, parameter in module.named_parameters():\n","        if not parameter.requires_grad:\n","            freezed_parameters.append(name)\n","            \n","    return freezed_parameters\n","\n","# # Remove unicode error (https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313330)\n","# def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n","#     return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n","\n","\n","# def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n","#     return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n","\n","# # Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n","# codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n","# codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n","\n","# def resolve_encodings_and_normalize(text: str) -> str:\n","#     \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n","#     text = (\n","#         text.encode(\"raw_unicode_escape\")\n","#         .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","#         .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n","#         .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n","#     )\n","#     text = unidecode(text)\n","#     return text"]},{"cell_type":"markdown","metadata":{"id":"dtRiSL4aelC8"},"source":["# MODELLING\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"x0oYnmlqbYJp"},"source":["## Dataset\n","\n","---\n","\n","Used to convert raw text into tokenized data"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.148726Z","iopub.status.busy":"2022-08-23T13:38:55.147949Z","iopub.status.idle":"2022-08-23T13:38:55.165263Z","shell.execute_reply":"2022-08-23T13:38:55.164219Z","shell.execute_reply.started":"2022-08-23T13:38:55.148683Z"},"id":"M553vuuRbYJp","trusted":true},"outputs":[],"source":["class _Dataset(Dataset):\n","    def __init__(self,data_path,test_path, tokenizer,label_encoder,attributes,config, max_token_len: int = 512, is_train=True,is_test=False):\n","        self.data_path = data_path\n","        self.test_path = test_path\n","        self.tokenizer = tokenizer\n","        self.attributes = attributes\n","        self.max_token_len = max_token_len\n","        self.is_train = is_train\n","        self.is_test = is_test\n","        self.label_encoder = label_encoder\n","        self.config = config\n","        self._prepare_data()\n","\n","    def _prepare_data(self):\n","        SEP = self.tokenizer.sep_token # different model uses different to text as seperator (e.g. [SEP], </s>)\n","        if self.is_test:\n","            df = self.test_path\n","            # df['discourse_text'] = df['discourse_text'].apply(resolve_encodings_and_normalize)\n","            df['text'] = df['discourse_type'] + SEP + df['discourse_text']\n","            try:\n","              # Validation use\n","              df = df.loc[:,['text','labels']]\n","            except:\n","              # Test use\n","              df = df.loc[:,['text']]\n","        else:\n","            df = self.data_path\n","            if self.config['param']['sample_mode']:\n","                df = df.sample(self.config['param']['sample_size'])\n","            y = df['discourse_effectiveness']\n","\n","            train_df, val_df = train_test_split(df, test_size=0.2,stratify=y,random_state=91)\n","\n","            if self.is_train:\n","                df = train_df.copy()  \n","            else:\n","                df = val_df.copy()\n","\n","            # df['discourse_text'] = df['discourse_text'].apply(resolve_encodings_and_normalize)\n","            df['text'] = df['discourse_type'] + SEP + df['discourse_text']\n","            df = df.rename(columns={'discourse_effectiveness':'labels'})\n","            df = df.loc[:,['text','labels']]\n","      \n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self,index):\n","        item = self.df.iloc[index]\n","        text = str(item.text)\n","        tokens = self.tokenizer.encode_plus(text,\n","                                  add_special_tokens= True,\n","                                  return_tensors='pt',\n","                                  truncation=True,\n","      #                                   padding='max_length',\n","                                  max_length=self.max_token_len,\n","                                  return_attention_mask = True)\n","        if self.is_test:\n","            return {'input_ids':tokens.input_ids.flatten(),'attention_mask': tokens.attention_mask.flatten()}\n","        else:\n","            # # Convert strings to numerics, follow alphabetical order\n","            attributes = item['labels'].split()\n","            self.label_encoder.fit(self.attributes)\n","            attributes = self.label_encoder.transform(attributes)\n","            attributes = torch.as_tensor(attributes)\n","            #         attributes = torch.FloatTensor(item[self.attributes])\n","            return {'input_ids':tokens.input_ids.flatten(),'attention_mask': tokens.attention_mask.flatten(), 'labels':attributes}\n"]},{"cell_type":"markdown","metadata":{"id":"XeHIWmVHelC-"},"source":["## Collate (Dynamic Padding)\n","\n","---\n","\n","Dynamically pad tokenized text to match the max length of each batch to reduce computational time"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.166957Z","iopub.status.busy":"2022-08-23T13:38:55.166558Z","iopub.status.idle":"2022-08-23T13:38:55.181694Z","shell.execute_reply":"2022-08-23T13:38:55.180741Z","shell.execute_reply.started":"2022-08-23T13:38:55.166918Z"},"id":"54bAri16elC-","trusted":true},"outputs":[],"source":["class Collate:\n","    def __init__(self, tokenizer, isTrain=True):\n","        self.tokenizer = tokenizer\n","        self.isTrain = isTrain\n","        # self.args = args\n","\n","    def __call__(self, batch):\n","        output = dict()\n","        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n","        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n","        if self.isTrain:\n","            output[\"labels\"] = [sample[\"labels\"] for sample in batch]\n","\n","        # calculate max token length of this batch\n","        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n","\n","        # add padding\n","        if self.tokenizer.padding_side == \"right\":\n","            output[\"input_ids\"] = [s.tolist() + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n","            output[\"attention_mask\"] = [s.tolist() + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n","\n","        else:\n","            output[\"input_ids\"] = [torch.FloatTensor((batch_max - len(s)) * [self.tokenizer.pad_token_id].tolist()) + s.tolist() for s in output[\"input_ids\"]]\n","            output[\"attention_mask\"] = [torch.FloatTensor((batch_max - len(s)) * [0]) + s.tolist() for s in output[\"attention_mask\"]]\n","            \n","        # convert to tensors\n","        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n","        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n","        if self.isTrain:\n","            output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.long)\n","        return output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N3ZT1vpFbYJq"},"source":["## Data Module\n","\n","---\n","\n","Data preparation by calling dataset and passing it to the dataloader where the data is collated and batched"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.184020Z","iopub.status.busy":"2022-08-23T13:38:55.183357Z","iopub.status.idle":"2022-08-23T13:38:55.198243Z","shell.execute_reply":"2022-08-23T13:38:55.197225Z","shell.execute_reply.started":"2022-08-23T13:38:55.183956Z"},"id":"_pUquMv6bYJq","trusted":true},"outputs":[],"source":["class _Data_Module(pl.LightningDataModule):\n","\n","    def __init__(self, data_path, test_path,attributes,label_encoder,tokenizer,config, batch_size: int = 8, max_token_length: int = 512):\n","        super().__init__()\n","        self.data_path = data_path\n","        self.test_path = test_path\n","        self.attributes = attributes\n","        self.batch_size = batch_size\n","        self.max_token_length = max_token_length\n","        # self.model_name = model_name\n","        self.tokenizer = tokenizer #AutoTokenizer.from_pretrained(model_name)\n","        self.label_encoder = label_encoder\n","        self.config = config\n","\n","    def setup(self, stage = None):\n","        if stage in (None, \"fit\"):\n","            self.train_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder,  attributes=self.attributes, is_train=True, tokenizer=self.tokenizer,config = self.config)\n","            self.val_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder, attributes=self.attributes, is_train=False,  tokenizer=self.tokenizer,config = self.config)\n","        if stage == 'predict':\n","            self.test_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder, attributes=self.attributes, is_train=False,is_test=True, tokenizer=self.tokenizer,config = self.config)\n","\n","\n","    def train_dataloader(self):\n","        collate_fn = Collate(self.tokenizer, \n","                           isTrain=True)\n","\n","        return DataLoader(self.train_dataset, \n","                        batch_size = self.batch_size, \n","                        num_workers=2, \n","                        shuffle=True,\n","                        collate_fn = collate_fn)\n","\n","    def val_dataloader(self):\n","        collate_fn = Collate(self.tokenizer, \n","                           isTrain=True)\n","\n","        return DataLoader(self.val_dataset, \n","                        batch_size = self.batch_size, \n","                        num_workers=2, \n","                        shuffle=False,\n","                        collate_fn = collate_fn)\n","\n","    def predict_dataloader(self):\n","        collate_fn = Collate(self.tokenizer, \n","                           isTrain=False)\n","\n","        return DataLoader(self.test_dataset, \n","                        batch_size = self.batch_size, \n","                        num_workers=2, \n","                        shuffle=False,\n","                        collate_fn = collate_fn)\n"]},{"cell_type":"markdown","metadata":{"id":"hGuHN2DqbYJq"},"source":["## Classifier\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3twx77pselDD"},"source":["### DistilBert classifier"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.200524Z","iopub.status.busy":"2022-08-23T13:38:55.200145Z","iopub.status.idle":"2022-08-23T13:38:55.229670Z","shell.execute_reply":"2022-08-23T13:38:55.228720Z","shell.execute_reply.started":"2022-08-23T13:38:55.200493Z"},"id":"jBzhFi4ZelDE","trusted":true},"outputs":[],"source":["class DistilBert_Text_Classifier(pl.LightningModule):\n","    \n","    def __init__(self, config: dict,data_module):\n","        super().__init__()\n","        self.config = config\n","        self.data_module=data_module\n","        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n","        freeze((self.pretrained_model).embeddings)\n","        freeze((self.pretrained_model).transformer.layer[:config['param']['n_freeze']])\n","#         print(get_freezed_parameters(self.pretrained_model))\n","        # Adding an additional hidden layer on top of the pretrained model\n","        # self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size,self.pretrained_model.config.hidden_size)\n","#         self.hidden2 = torch.nn.Linear(self.pretrained_model.config.hidden_size,100)\n","\n","#         self.batchnorm = nn.BatchNorm1d(self.pretrained_model.config.hidden_size)\n","        # Adding classifier on top of the pretrained model\n","        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['param']['n_labels'])\n","        \n","        # Used to initialize the weight of the newly created classifier layer, not sure whether hidden layer need it or not\n","        # torch.nn.init.xavier_uniform_(self.classifier.weight)\n","        \n","        self.loss_func = nn.CrossEntropyLoss() # do not put SoftMax, just use CrossEntropyLoss\n","        \n","        self.dropout = nn.Dropout(config['param']['p_dropout'])\n","\n","    # For inference        \n","    def forward(self, input_ids, attention_mask, labels = None):\n","        output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n","        # torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) — Sequence of hidden-states at the output of the last layer of the model\n","        # example: output.last_hidden_state -> torch.Size([64, 277, 768])\n","        # torch.mean(output.last_hidden_state, 1) -> torch.Size([64, 768])\n","        pooled_output = torch.mean(output.last_hidden_state, 1)  # mean of sequence length\n","        pooled_output = F.relu(pooled_output)\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        # pooled_output = self.hidden(pooled_output)\n","        # pooled_output = F.relu(pooled_output)\n","#         pooled_output = self.batchnorm(pooled_output)\n","        # pooled_output = self.dropout(pooled_output)\n","        # logits = self.classifier(pooled_output)\n","            \n","        loss = 0\n","        if labels is not None:\n","            loss = self.loss_func(logits,labels)\n","        return loss, logits\n","\n","\n","#     def training_step(self, batch, batch_index):\n","#         logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n","# #         print(f\"batch[labels] = {batch['labels']}\")\n","# #         print(f\"{type(batch['labels'])}\")\n","#         class_weights=class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(batch['labels'].tolist()),y=batch['labels'].tolist())\n","#         class_weights=torch.tensor(class_weights,dtype=torch.float)\n","#         loss_func = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to('cuda:0')\n","#         loss = loss_func(logits,batch['labels']).to('cuda:0')\n","        \n","#         f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n","#         f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n","#         wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n","#         self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n","#         self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n","#         self.log(\"loss \", loss,on_step=False,on_epoch = True, prog_bar = True, logger=True)\n","#         return {\"loss\":loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n","    \n","#     def validation_step(self, batch, batch_index):\n","#         logits = self(**batch)\n","# #         print(f\"batch[labels] = {batch['labels']}\")\n","#         class_weights=class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(batch['labels'].tolist()),y=batch['labels'].tolist())\n","#         class_weights=torch.tensor(class_weights,dtype=torch.float)\n","#         loss_func = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to('cuda:0')\n","#         loss = loss_func(logits,batch['labels']).to('cuda:0')\n","        \n","        \n","#         f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n","#         f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n","#         wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n","#         self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n","#         self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n","#         self.log(\"val_loss\", loss, on_step=False,on_epoch = True, prog_bar = True, logger=True)\n","#         return {\"val_loss\": loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n","#     \n","#     def predict_step(self, batch, batch_index):\n","#         logits = self(**batch)\n","#         return logits\n","\n","    def training_step(self, batch, batch_index):\n","        loss, logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n","        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n","        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n","        # wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n","        self.log(\"f1\", f1, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n","        self.log(\"f1_weighted\", f1_weighted, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n","        self.log(\"loss\", loss,on_step=True,on_epoch = True, prog_bar = True, logger=True)\n","        return {\"loss\":loss,\"f1\":f1,\"f1_weighted\":f1_weighted}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n","    \n","    def training_epoch_end(self, outputs):\n","        loss = outputs[0]['loss'].item()\n","        f1 = outputs[0]['f1'].item()\n","        f1_weighted = outputs[0]['f1_weighted'].item()\n","        if self.config['wandb']:\n","            wandb.log({\"Train Loss\": loss,'Train F1':f1,'Train F1 Weighted':f1_weighted})\n","        return\n","\n","    def validation_step(self, batch, batch_index):\n","        loss, logits = self(**batch)\n","        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n","        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n","        # wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n","        self.log(\"f1\", f1, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n","        self.log(\"f1_weighted\", f1_weighted, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n","        self.log(\"val_loss\", loss, on_step=True,on_epoch = True, prog_bar = True, logger=True)\n","        return {\"val_loss\": loss,\"val_f1\":f1,\"val_f1_weighted\":f1_weighted}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n","    \n","    def validation_epoch_end(self, outputs):\n","        loss = outputs[0]['val_loss'].item()\n","        f1 = outputs[0]['val_f1'].item()\n","        f1_weighted = outputs[0]['val_f1_weighted'].item()\n","        if self.config['wandb']:\n","            wandb.log({\"Val Loss\": loss,'Val F1':f1,'Val F1 Weighted':f1_weighted})\n","        return\n","\n","    def predict_step(self, batch, batch_index):\n","        loss, logits = self(**batch)\n","        return logits\n","    \n","#     def on_train_end(self):\n","#         AutoModel.save_pretrained(logits)\n","    \n","    def configure_optimizers(self):\n","        train_size = len(self.data_module.train_dataloader())\n","        \n","        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config['param']['lr'], weight_decay=self.config['param']['weight_decay'])\n","        if self.config['param']['scheduler']:\n","            total_steps = train_size/self.config['param']['batch_size']\n","            warmup_steps = math.floor(total_steps * self.config['param']['warmup'])\n","            scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","            return[optimizer],[scheduler]\n","        else:\n","            return optimizer"]},{"cell_type":"markdown","metadata":{"id":"rGzFc2J_elDH"},"source":["# TRAINING & VALIDATION\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"vVg3xt2DelDH"},"source":["## Training Function"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.231425Z","iopub.status.busy":"2022-08-23T13:38:55.231107Z","iopub.status.idle":"2022-08-23T13:38:55.244367Z","shell.execute_reply":"2022-08-23T13:38:55.243595Z","shell.execute_reply.started":"2022-08-23T13:38:55.231394Z"},"id":"ypMDoU4telDI","trusted":true},"outputs":[],"source":["def train(config,Text_Classifier,project, notes,timestr):\n","    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n","    le = LabelEncoder()\n","    \n","    data_module = _Data_Module(data_path,\n","                                    test_path,\n","                                    attributes,\n","                                    le,\n","                                    tokenizer,\n","                                    # config['model_name'],\n","                                    batch_size=config['param']['batch_size'],\n","                                    config = config\n","                                   )\n","    data_module.setup()\n","    \n","    # model\n","    model = Text_Classifier(config,data_module)\n","\n","    # trainer and fit\n","    if config['param']['swa']:\n","        trainer = pl.Trainer(max_epochs=config['param']['n_epochs'],\n","                             accelerator='auto',\n","                             callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\",patience = 3),TQDMProgressBar(refresh_rate=30),StochasticWeightAveraging(swa_lrs=config['param']['swa_lrs'])],\n","                             default_root_dir=\"./checkpoints\",\n","                             deterministic=True, # To ensure reproducibility\n","                             precision = config['param']['precision'],\n","                            ) # automatic mixed precision to reduce memory\n","    else:\n","        trainer = pl.Trainer(max_epochs=config['param']['n_epochs'],\n","                             accelerator='auto',\n","                             callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\",patience = 3),TQDMProgressBar(refresh_rate=30)],\n","                             default_root_dir=\"./checkpoints\",\n","                             deterministic=True, # To ensure reproducibility\n","                             precision = config['param']['precision'],\n","                            ) # automatic mixed precision to reduce memory\n","\n","    if config['wandb']:\n","        run = wandb.init(name = f\"{str(config['param'])}\",\n","                      notes = str(config) + notes,\n","                      project=project,\n","                        tags = timestr)\n","\n","        trainer.fit(model, data_module)\n","\n","        run.finish()\n","    else:\n","        trainer.fit(model, data_module)\n","    \n","    try:\n","        tuned_model_path = f\"{TRAINED_DIR}{str(config['param'])}_{timestr}.pth\"\n","        # tuned_model_path = f\"/content/drive/MyDrive/Colab Notebooks/FineTuneModel/{config['name']}_E{config['n_epochs']}Size{config['batch_size']}Lr{config['lr']}Warm{config['warmup']}Weight{config['weight_decay']}Freeze{config['n_freeze']}Drop{config['p_dropout']}Text{config['text_method']}_{timestr}.pth\"\n","        config['newly_tuned_model_path'] = tuned_model_path\n","        torch.save(model.state_dict(), tuned_model_path)\n","    except:\n","        try:\n","            tuned_model_path = f\"{TRAINED_DIR}{timestr}.pth\"\n","            config['newly_tuned_model_path'] = tuned_model_path\n","            torch.save(model.state_dict(), tuned_model_path)\n","        except:\n","            print(\"Failed to save model\")\n","\n","## Note: https://huggingface.co/transformers/v1.0.0/model_doc/overview.html\n","    # Convert model to huggingface compatible model\n","#     state_dict = torch.load(config['newly_tuned_model_path'])\n","#     model_hf = AutoModel.from_pretrained(config['model_name'],state_dict=state_dict)\n","#     model_hf.save_pretrained(f\"{TRAINED_DIR}{config['param']}_{timestr}\")\n","    return model,config\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.246436Z","iopub.status.busy":"2022-08-23T13:38:55.245927Z","iopub.status.idle":"2022-08-23T13:38:55.259024Z","shell.execute_reply":"2022-08-23T13:38:55.257933Z","shell.execute_reply.started":"2022-08-23T13:38:55.246394Z"},"trusted":true},"outputs":[],"source":["# attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n","# #     config['model_name'] = config['existing_tuned_model_name']\n","# tokenizer = AutoTokenizer.from_pretrained(distilbert_config['model_name'], use_fast=True)\n","# le = LabelEncoder()\n","\n","#     # Initialize data module\n","# test_data_module = _Data_Module(data_path,\n","#                                     test_path,\n","#                                     attributes,\n","#                                     le,\n","#                                     tokenizer,\n","#                                     batch_size=distilbert_config['param']['batch_size'],\n","#                                     config=distilbert_config\n","#                                    )\n","# test_data_module.setup()\n","\n","#     # Initialize Model\n","# model = DistilBert_Text_Classifier(distilbert_config,test_data_module)\n","# model.load_state_dict(torch.load(distilbert_config['newly_tuned_model_path']))\n","\n","# # # state_dict = torch.load(distilbert_config['newly_tuned_model_path'])\n","# # model_hf = AutoModel.from_pretrained(distilbert_config['model_name'])\n","# # # model_hf.save_pretrained(f\"{TRAINED_DIR}{distilbert_config['param']}_{timestr}\",state_dict=state_dict)\n","# # model_hf.load_state_dict(model)\n","# # model_hf = AutoModel.from_pretrained(\"./{'n_labels': 3, 'batch_size': 64, 'lr': 0.0008, 'warmup': 0, 'weight_decay': 0.01, 'n_epochs': 5, 'n_freeze': 5, 'p_dropout': 0, 'scheduler': False, 'precision': 16, 'sample_mode': True, 'sample_size': 100, 'swa': False, 'swa_lrs': 0.01}_20220822-011207\")"]},{"cell_type":"markdown","metadata":{},"source":["## Validation Function"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.260871Z","iopub.status.busy":"2022-08-23T13:38:55.260550Z","iopub.status.idle":"2022-08-23T13:38:55.273603Z","shell.execute_reply":"2022-08-23T13:38:55.272804Z","shell.execute_reply.started":"2022-08-23T13:38:55.260842Z"},"trusted":true},"outputs":[],"source":["def validate(_Text_Classifier,config,data_path,val_path,attributes,timestr):\n","    wandb = False\n","    if config['wandb']:\n","        wandb=True\n","        config['wandb']=False\n","        \n","    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n","    le = LabelEncoder()\n","    \n","    val_data_module = _Data_Module(data_path,\n","                                    val_path, # using \n","                                    attributes,\n","                                    le,\n","                                    tokenizer,\n","                                    batch_size=config['param']['batch_size'],\n","                                    config = config\n","                                   )   \n","\n","    val_data_module.setup()\n","\n","    # Initialize Model\n","    model = _Text_Classifier(config,val_data_module)\n","    model.load_state_dict(torch.load(config['newly_tuned_model_path']))\n","\n","    # Initialize Trainer\n","    trainer = pl.Trainer(accelerator='auto')\n","   \n","    logits = trainer.predict(model, datamodule=val_data_module)\n","    \n","    pred_list = []\n","    for logit in logits:\n","        pred_list.append(logit)\n","    y_pred = torch.cat(pred_list)\n","\n","    argmax_output = y_pred.argmax(dim=1)\n","    argmax_output = argmax_output.numpy()\n","    \n","    val_df = val_path.copy()\n","    output_df = pd.concat([val_df.reset_index(drop=True), pd.DataFrame(argmax_output,columns=['pred_discourse_effectiveness'])], axis=1)\n","    output_df['pred_discourse_effectiveness'] = output_df['pred_discourse_effectiveness'].map({0:'Adequate',1:'Effective',2:'Ineffective'})\n","    df = pd.concat([output_df.reset_index(drop=True),pd.DataFrame(y_pred.tolist(),columns=attributes)],axis=1)\n","\n","    df.to_csv(f'{TRAINED_DIR}distilbert_valresult_{timestr}.csv',index=False)\n","    if wandb:\n","        config['wandb']=True\n","        wandb=False\n","    return output_df,y_pred\n","\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.275533Z","iopub.status.busy":"2022-08-23T13:38:55.274658Z","iopub.status.idle":"2022-08-23T13:38:55.342135Z","shell.execute_reply":"2022-08-23T13:38:55.340937Z","shell.execute_reply.started":"2022-08-23T13:38:55.275502Z"},"trusted":true},"outputs":[],"source":["## Validation Configuration\n","\n","attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n","\n","# Create validation csv file from traintestsplit\n","df = data_path.copy()\n","y = df['discourse_effectiveness']\n","train_df,val_df = train_test_split(df, test_size=0.2,stratify=y,random_state=91)\n","\n","# Run a smaller validation data set as an example. Actual validation was ran using Kaggle notebook and saved as csv\n","# y = val_df['discourse_effectiveness']\n","# train_df,val_df = train_test_split(val_df, test_size=0.01,stratify=y,random_state=91)\n","\n","val_path = val_df\n","val_path['discourse_effectiveness'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["## Validation Utility"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.344011Z","iopub.status.busy":"2022-08-23T13:38:55.343653Z","iopub.status.idle":"2022-08-23T13:38:55.358320Z","shell.execute_reply":"2022-08-23T13:38:55.357272Z","shell.execute_reply.started":"2022-08-23T13:38:55.343980Z"},"trusted":true},"outputs":[],"source":["#Convert prediction in tensor format to argmax numpy format\n","def pred_to_argmax(y_pred,df,model_name):\n","    argmax_output = y_pred.argmax(dim=1)\n","    argmax_output = argmax_output.numpy()\n","    df = pd.concat([df.reset_index(drop=True),pd.DataFrame(argmax_output,columns=[f'pred_effectiveness_{model_name}'])], axis=1)\n","    df[f'pred_effectiveness_{model_name}'] = df[f'pred_effectiveness_{model_name}'].map({0:'Adequate',1:'Effective',2:'Ineffective'})\n","    return df\n","\n","#Plot confusion matrix\n","def do_conf_matrix(y_true, y_pred, ax, title=None):\n","    cm = confusion_matrix(y_true, y_pred, labels=attributes)\n","    sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', ax=ax, cmap='Blues');  \n","    \n","    \n","    # labels, title and ticks\n","    ax.set_xlabel('Predicted labels');\n","    ax.set_ylabel('True labels'); \n","    ax.set_title(f'Confusion Matrix - {title}'); \n","\n","    ax.xaxis.set_ticklabels(attributes)\n","    ax.yaxis.set_ticklabels(attributes);\n","    plt.show()\n","# Calculate metrics\n","def calculate_score(y_pred,df):\n","    cel = nn.CrossEntropyLoss()\n","    f1 = f1_score(y_pred.argmax(dim=1),torch.tensor(df['discourse_effectiveness_numeric'].values),num_classes=3,average=None)\n","    f1_weighted = f1_score(y_pred.softmax(dim=1),torch.tensor(df['discourse_effectiveness_numeric'].values),num_classes=3,multiclass=True,average='weighted')\n","    loss = cel(y_pred,torch.tensor(df['discourse_effectiveness_numeric'].values))\n","    \n","    return f1, f1_weighted, loss\n","\n","def validate_score(y_pred,val_path,model_name):\n","    df = val_path.copy()\n","    df['discourse_effectiveness_numeric'] = df['discourse_effectiveness'].map({'Adequate':0,'Effective':1,'Ineffective':2})\n","    y_true = df['discourse_effectiveness'].values\n","    # fig, axs = plt.subplots(2,2,figsize=(20, 10))\n","\n","    fig, axs = plt.subplots(1,1,figsize=(10, 10))\n","    df = pred_to_argmax(y_pred,df,model_name)\n","\n","    do_conf_matrix(y_true, df[f'pred_effectiveness_distilbert'].values, ax=axs,title = model_name)\n","\n","    output = calculate_score(y_pred,df)\n","\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"fhwoVxJNrxdb"},"source":["## Execute Training & Validation"]},{"cell_type":"markdown","metadata":{},"source":["https://pytorch-lightning.readthedocs.io/en/latest/advanced/training_tricks.html"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.360534Z","iopub.status.busy":"2022-08-23T13:38:55.359920Z","iopub.status.idle":"2022-08-23T13:38:55.374081Z","shell.execute_reply":"2022-08-23T13:38:55.373236Z","shell.execute_reply.started":"2022-08-23T13:38:55.360502Z"},"id":"EidUj0OhelDJ","outputId":"01d67893-452f-4aea-e98f-77dbb79e6817","trusted":true},"outputs":[],"source":["for iteration in range(1):\n","    timestr = time.strftime(\"%Y%m%d-%H%M%S\")   \n","    \n","    if iteration == 0:\n","        distilbert_config['param']['lr']= 5e-5\n","        distilbert_config['param']['sample_mode']= False\n","        distilbert_config['param']['precision']= 16\n","        distilbert_config['param']['n_freeze']= 4\n","        distilbert_config['param']['n_epochs']= 10\n","        distilbert_config['param']['weight_decay']=0\n","        distilbert_config['param']['p_dropout'] = 0.4\n","        distilbert_config['param']['scheduler']=True\n","        distilbert_config['param']['warmup']=0.3\n","        distilbert_config['param']['swa']=False\n","        distilbert_config['param']['swa_lrs']=0.1\n","        \n","\n","            \n","    print(f'Iteration {iteration} : {str(distilbert_config)}')\n","    ## Train\n","    distilbert_model, distilbert_config  = train(config = distilbert_config,\n","                                        Text_Classifier = DistilBert_Text_Classifier,\n","                                        project = 'DistilBert_Text_Classifier',\n","                                        timestr = timestr,\n","                                        notes = \n","                                                  \"\"\"\n","                                                output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask), \n","                                                pooled_output = torch.mean(output.last_hidden_state, 1), \n","                                                pooled_output = F.relu(pooled_output), \n","                                                pooled_output = self.dropout(pooled_output)\n","                                                logits = self.classifier(pooled_output), \n","                                                dynamic collate = True\n","                                                  \"\"\")\n","    \n","    ## Validate last run\n","    val_output_df_distilbert, y_pred_distilbert = validate(DistilBert_Text_Classifier,\n","                                                      distilbert_config,\n","                                                      data_path,\n","                                                      val_path,\n","                                                      attributes,\n","                                                      timestr)\n","    # Visualize scores\n","    val_result=pd.read_csv(f'{TRAINED_DIR}distilbert_valresult_{timestr}.csv')\n","    y_pred = torch.from_numpy(val_result.loc[:,['Adequate','Effective','Ineffective']].values)\n","    \n","    output = validate_score(y_pred,val_path,'distilbert')\n","    \n","    f1_score_df = pd.DataFrame({'distilbert':output[0].tolist()})\n","    f1_score_df['label']=attributes\n","    f1_score_df.set_index(['label'])\n","    f1_weight_score = []\n","    f1_weight_score.append(output[1].item())\n","    logloss_score = []\n","    logloss_score.append(output[2].item())\n","\n","    score_df = pd.DataFrame({'f1_weighted_score':f1_weight_score})\n","    score_df['log loss'] = logloss_score\n","    score_df['model']='distilbert'\n","    print(score_df)\n"]},{"cell_type":"markdown","metadata":{"id":"jnvJg9SLty00"},"source":["# PREDICTION AND KAGGLE SUBMISSION\n","---\n","\n","https://www.kaggle.com/competitions/feedback-prize-effectiveness\n"]},{"cell_type":"markdown","metadata":{"id":"vSfLG9yGelDV"},"source":["## Main function to execute prediction on test dataset"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.376020Z","iopub.status.busy":"2022-08-23T13:38:55.375700Z","iopub.status.idle":"2022-08-23T13:38:55.389939Z","shell.execute_reply":"2022-08-23T13:38:55.389165Z","shell.execute_reply.started":"2022-08-23T13:38:55.375993Z"},"id":"RgBhridfs8uF","trusted":true},"outputs":[],"source":["def predict(_Text_Classifier,config):\n","    attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n","#     config['model_name'] = config['existing_tuned_model_name']\n","    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n","    le = LabelEncoder()\n","\n","    # Initialize data module\n","    test_data_module = _Data_Module(data_path,\n","                                    test_path,\n","                                    attributes,\n","                                    le,\n","                                    tokenizer,\n","                                    batch_size=config['param']['batch_size'],\n","                                    config=config\n","                                   )\n","    test_data_module.setup()\n","\n","    # Initialize Model\n","    model = _Text_Classifier(config,test_data_module)\n","    model.load_state_dict(torch.load(config['newly_tuned_model_path']))\n","\n","    # Initialize Trainer\n","    trainer = pl.Trainer(accelerator='auto')\n","\n","    # Run predictions\n","    def predict_text_classification(model, dm):\n","        predictions = trainer.predict(model, datamodule=dm)\n","        return predictions\n","    predictions = predict_text_classification(model, test_data_module)\n","\n","    # Pass logit into a softmax\n","    pred_list = []\n","    for logits in predictions:\n","        pred_list.append(logits)\n","    y_pred = torch.cat(pred_list)\n","    y_pred.shape\n","\n","    softmax_outputs = softmax(y_pred, axis=1)\n","    test_df = test_path.copy()\n","    output_df = pd.concat([test_df[['discourse_id']].reset_index(drop=True), pd.DataFrame(softmax_outputs.numpy(), columns=attributes)], axis=1)\n","#     output_df = pd.concat([test_df[['discourse_id']].reset_index(drop=True), pd.DataFrame(y_pred.numpy(), columns=attributes)], axis=1)\n","    new_cols = [\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"]\n","    output_df = output_df[new_cols]\n","\n","    return output_df, y_pred"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:55.391901Z","iopub.status.busy":"2022-08-23T13:38:55.391471Z","iopub.status.idle":"2022-08-23T13:38:57.544861Z","shell.execute_reply":"2022-08-23T13:38:57.543942Z","shell.execute_reply.started":"2022-08-23T13:38:55.391845Z"},"trusted":true},"outputs":[],"source":["\n","df, y_pred = predict(DistilBert_Text_Classifier,distilbert_config)\n","\n","softmax_outputs = softmax(y_pred, axis=1)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:57.547636Z","iopub.status.busy":"2022-08-23T13:38:57.546514Z","iopub.status.idle":"2022-08-23T13:38:57.556626Z","shell.execute_reply":"2022-08-23T13:38:57.555528Z","shell.execute_reply.started":"2022-08-23T13:38:57.547596Z"},"trusted":true},"outputs":[],"source":["# Create dataframe for submission\n","output_df = pd.concat([test_path[['discourse_id']].reset_index(drop=True), pd.DataFrame(softmax_outputs.numpy(), columns=attributes)], axis=1)\n","#     output_df = pd.concat([test_df[['discourse_id']].reset_index(drop=True), pd.DataFrame(y_pred.numpy(), columns=attributes)], axis=1)\n","\n","# Re-arrange columns\n","new_cols = [\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"]\n","output_df = output_df[new_cols]"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-08-23T13:38:57.558668Z","iopub.status.busy":"2022-08-23T13:38:57.557806Z","iopub.status.idle":"2022-08-23T13:38:57.583738Z","shell.execute_reply":"2022-08-23T13:38:57.583018Z","shell.execute_reply.started":"2022-08-23T13:38:57.558635Z"},"trusted":true},"outputs":[],"source":["output_df.to_csv('submission.csv', index=False)\n","pd.read_csv('submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('dsi-sg')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"9f8da1e1bd357d59a3b66de042951529b522660e89c8368a5e504d03fc911be3"}}},"nbformat":4,"nbformat_minor":4}
