{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "2-modelling-evaluation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kitkeat/Effective-Argument-Prediction-NLP/blob/main/2-modelling-evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ! pip install accelerate nvidia-ml-py3\n",
        "# ! pip install datasets==2.1.0\n",
        "# ! pip install transformers==4.18.0\n",
        "# ! pip install sentencepiece==0.1.96\n",
        "# ! pip install pytorch-lightning==1.6.5\n",
        "# ! pip install torchmetrics==0.9.2\n",
        "# ! pip install wandb==0.12.21"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "wMqf8jrybYJf",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:06.301478Z",
          "iopub.execute_input": "2022-08-03T12:57:06.302153Z",
          "iopub.status.idle": "2022-08-03T12:57:06.333004Z",
          "shell.execute_reply.started": "2022-08-03T12:57:06.302027Z",
          "shell.execute_reply": "2022-08-03T12:57:06.331457Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        os.path.join(dirname, filename)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option(\"max_colwidth\", None)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import re\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW, get_cosine_schedule_with_warmup, EarlyStoppingCallback, AutoModel\n",
        "from datasets import Dataset, Value, ClassLabel, Features, load_metric\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.checkpoint import checkpoint # need to call when using gradient_checkpointing\n",
        "\n",
        "from sklearn.metrics import log_loss, confusion_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from scipy.special import softmax\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from pytorch_lightning.callbacks import EarlyStopping, TQDMProgressBar\n",
        "from torchmetrics.functional import f1_score\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "import wandb\n",
        "\n",
        "from text_unidecode import unidecode\n",
        "from typing import Dict, List, Tuple\n",
        "import codecs"
      ],
      "metadata": {
        "id": "_Qo3zsKzbYJi",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:07.957068Z",
          "iopub.execute_input": "2022-08-03T12:57:07.957534Z",
          "iopub.status.idle": "2022-08-03T12:57:23.489128Z",
          "shell.execute_reply.started": "2022-08-03T12:57:07.957491Z",
          "shell.execute_reply": "2022-08-03T12:57:23.487742Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU availability"
      ],
      "metadata": {
        "id": "52xPquzWeVpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "gYfDzL4vbYJi",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:23.566611Z",
          "iopub.execute_input": "2022-08-03T12:57:23.566954Z",
          "iopub.status.idle": "2022-08-03T12:57:23.576544Z",
          "shell.execute_reply.started": "2022-08-03T12:57:23.566922Z",
          "shell.execute_reply": "2022-08-03T12:57:23.575606Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Login to Weight and Bias"
      ],
      "metadata": {
        "id": "rOX_Y3l4eVpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "try:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    secret_value = user_secrets.get_secret(\"wand_api\")\n",
        "    !wandb login {secret_value}\n",
        "except:\n",
        "    print(\"wandb failed to login...\")\n",
        "    \n"
      ],
      "metadata": {
        "id": "ZvmYfktWDgpz",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:23.581113Z",
          "iopub.execute_input": "2022-08-03T12:57:23.581498Z",
          "iopub.status.idle": "2022-08-03T12:57:26.679144Z",
          "shell.execute_reply.started": "2022-08-03T12:57:23.581448Z",
          "shell.execute_reply": "2022-08-03T12:57:26.677602Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "R1I0N_MteVpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n",
        "\n",
        "\n",
        "data_path = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\n",
        "test_path = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\n",
        "\n",
        "deberta_config={'name':'deberta',\n",
        "                'model_name': '../input/deberta-v3-base/deberta-v3-base',\n",
        "                'PATH' : '../input/debertav3basefinetuned3172022/deberta_E3Size16Lr3e-05Warm0.01Weight0.01Freeze2Drop0.1Text0.pth',\n",
        "                'n_labels': 3,\n",
        "                'batch_size': 16,\n",
        "                'lr': 3e-5,\n",
        "                'warmup': 0.01, \n",
        "                'weight_decay': 0.01,\n",
        "                'n_epochs': 4,\n",
        "                'n_freeze' : 2,#9,\n",
        "                'p_dropout':0.65,\n",
        "                'text_method': 2\n",
        "                }\n",
        "\n",
        "distilbert_config={'name': 'distilbert',\n",
        "                'model_name': '../input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad',\n",
        "                'PATH' : '../input/distilberttuned/distilbert-frozenembedding2transformlayer-5epoch-lr6e5-drop02.pth',\n",
        "                'n_labels': 3,\n",
        "                'batch_size': 64,\n",
        "                'lr': 8e-4,#6e-5,\n",
        "                'warmup': 0.2, \n",
        "                'weight_decay': 0.001,\n",
        "                'n_epochs': 5,#4,\n",
        "                'n_freeze' : 3,\n",
        "                'p_dropout':0.6,#0.2,#0.6,\n",
        "                'text_method': 1\n",
        "                }\n",
        "\n",
        "bertofa_config={'name':'bertofa',\n",
        "                'model_name': '../input/bertlargeuncasedsparse90unstructuredpruned/bert-large-uncased-sparse-90-unstructured-pruneofa',\n",
        "                'PATH' : '../input/bert-ofa-pretuned3072022/Bert_OFA_E3Size64Lr0.0001Warm02Weight1e-06Freeze21Drop001_full.pth',\n",
        "                'n_labels': 3,\n",
        "                'batch_size': 64,\n",
        "                'lr': 1e-4,\n",
        "                'warmup': 0.2, \n",
        "                'weight_decay': 1e-6,\n",
        "                'n_epochs': 3,\n",
        "                'n_freeze' : 21,\n",
        "                'p_dropout':0.1,\n",
        "                'text_method': 2\n",
        "                }\n",
        "\n",
        "distilroberta_config={'name':'distilroberta',\n",
        "                'model_name': '../input/distilrobertabase/distilroberta-base',\n",
        "                'PATH' : '../input/distilrobertafinetuned3072022/epoch7-step1840.pth',\n",
        "                'n_labels': 3,\n",
        "                'batch_size': 128,\n",
        "                'lr': 6e-5,\n",
        "                'warmup': 0.2, \n",
        "                'weight_decay': 0.001,\n",
        "                'n_epochs': 10,\n",
        "                'n_freeze' : 5,\n",
        "                'p_dropout':0,\n",
        "                'text_method': 2\n",
        "                }\n",
        "\n",
        "seed_everything(91, workers=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:26.681852Z",
          "iopub.execute_input": "2022-08-03T12:57:26.682304Z",
          "iopub.status.idle": "2022-08-03T12:57:27.022672Z",
          "shell.execute_reply.started": "2022-08-03T12:57:26.682268Z",
          "shell.execute_reply": "2022-08-03T12:57:27.021894Z"
        },
        "trusted": true,
        "id": "5unTmBrHeVpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility"
      ],
      "metadata": {
        "id": "NF12bOIkeVpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_essay(essay_id, is_train=True):\n",
        "    INPUT_DIR = '../input/feedback-prize-effectiveness/'\n",
        "    parent_path = INPUT_DIR + 'train' if is_train else INPUT_DIR + 'test'\n",
        "    \n",
        "    try:\n",
        "        essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
        "        essay_text = open(essay_path, 'r').read()\n",
        "    except:\n",
        "        parent_path = INPUT_DIR + 'train'\n",
        "        essay_path = os.path.join(parent_path, f\"{essay_id}.txt\")\n",
        "        essay_text = open(essay_path, 'r').read()        \n",
        "    return essay_text\n",
        "\n",
        "# Freeze the hidden layer within the pretrained model\n",
        "def freeze(module):\n",
        "    \"\"\"\n",
        "    Freezes module's parameters.\n",
        "    \"\"\"\n",
        "    \n",
        "    for parameter in module.parameters():\n",
        "        parameter.requires_grad = False\n",
        "        \n",
        "def get_freezed_parameters(module):\n",
        "    \"\"\"\n",
        "    Returns names of freezed parameters of the given module.\n",
        "    \"\"\"\n",
        "    \n",
        "    freezed_parameters = []\n",
        "    for name, parameter in module.named_parameters():\n",
        "        if not parameter.requires_grad:\n",
        "            freezed_parameters.append(name)\n",
        "            \n",
        "    return freezed_parameters\n",
        "\n",
        "# Remove unicode error (https://www.kaggle.com/competitions/feedback-prize-2021/discussion/313330)\n",
        "def replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n",
        "    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n",
        "\n",
        "\n",
        "def replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n",
        "    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n",
        "\n",
        "# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\n",
        "codecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\n",
        "codecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n",
        "\n",
        "def resolve_encodings_and_normalize(text: str) -> str:\n",
        "    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n",
        "    text = (\n",
        "        text.encode(\"raw_unicode_escape\")\n",
        "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
        "        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n",
        "        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n",
        "    )\n",
        "    text = unidecode(text)\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.023955Z",
          "iopub.execute_input": "2022-08-03T12:57:27.024682Z",
          "iopub.status.idle": "2022-08-03T12:57:27.039001Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.024650Z",
          "shell.execute_reply": "2022-08-03T12:57:27.037735Z"
        },
        "trusted": true,
        "id": "GaufRLd4eVpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELLING\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "D1XYxkMSeVpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "---\n",
        "\n",
        "Used to convert raw text into tokenized data"
      ],
      "metadata": {
        "id": "x0oYnmlqbYJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _Dataset(Dataset):\n",
        "    def __init__(self,data_path,test_path, tokenizer,label_encoder,attributes, max_token_len: int = 512, is_train=True,is_test=False, text_method=0):\n",
        "        self.data_path = data_path\n",
        "        self.test_path = test_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.attributes = attributes\n",
        "        self.max_token_len = max_token_len\n",
        "        self.is_train = is_train\n",
        "        self.is_test = is_test\n",
        "        self.label_encoder = label_encoder\n",
        "        self.text_method = text_method\n",
        "        self._prepare_data()\n",
        "\n",
        "    def _prepare_data(self):\n",
        "\n",
        "\n",
        "\n",
        "        SEP = self.tokenizer.sep_token # different model uses different to text as seperator (e.g. [SEP], </s>)\n",
        "        if self.is_test:\n",
        "#             df = pd.read_csv(self.test_path)\n",
        "            df = self.test_path\n",
        "            try:\n",
        "                df['essay_text']  = df['essay_id'].apply(lambda x: get_essay(x, is_train=False))\n",
        "            except:\n",
        "                print(\"Fail to get essay\")\n",
        "            df['discourse_text'] = df['discourse_text'].apply(resolve_encodings_and_normalize)\n",
        "#             df['discourse_text'] = df['discourse_text'].replace(r'\\n',' ', regex=True)\n",
        "            \n",
        "            try:\n",
        "                if self.text_method == 0:\n",
        "                    df['text'] = df['discourse_text']\n",
        "                elif self.text_method == 1:\n",
        "                    df['text'] = df['discourse_type'] + SEP + df['discourse_text']\n",
        "                elif self.text_method == 2:\n",
        "                    df['text'] = df['discourse_type'] + ' ' + df['discourse_text'] + SEP + df['essay_text'] # BERT was trained on 2 sentences\n",
        "            except:\n",
        "                df['text'] = df['discourse_text']\n",
        "                \n",
        "            try:\n",
        "                # Validation use\n",
        "                df = df.loc[:,['text','labels']]\n",
        "            except:\n",
        "                # Test use\n",
        "                df = df.loc[:,['text']]\n",
        "\n",
        "        else:\n",
        "#             df = pd.read_csv(self.data_path)\n",
        "            df = self.data_path\n",
        "            df = df.sample(1000)\n",
        "            try:\n",
        "                df['essay_text']  = df['essay_id'].apply(lambda x: get_essay(x, is_train=True))\n",
        "            except:\n",
        "                print('Fail to get essay')\n",
        "                \n",
        "            y = df['discourse_effectiveness']\n",
        "\n",
        "            train_df, val_df = train_test_split(df, test_size=0.2,stratify=y,random_state=91)\n",
        "\n",
        "            if self.is_train:\n",
        "                df = train_df.copy()\n",
        "            else:\n",
        "                df = val_df.copy()\n",
        "\n",
        "            df['discourse_text'] = df['discourse_text'].apply(resolve_encodings_and_normalize)\n",
        "#             df['discourse_text'] = df['discourse_text'].replace(r'\\n',' ', regex=True)\n",
        "            try:\n",
        "                if self.text_method == 0:\n",
        "                    df['text'] = df['discourse_text']\n",
        "                elif self.text_method == 1:\n",
        "                    df['text'] = df['discourse_type'] + SEP + df['discourse_text']\n",
        "                elif self.text_method == 2:\n",
        "                    df['text'] = df['discourse_type'] + ' ' + df['discourse_text'] + SEP + df['essay_text'] # BERT was trained on 2 sentences\n",
        "            except:\n",
        "                df['text'] = df['discourse_text']\n",
        "                \n",
        "            df = df.rename(columns={'discourse_effectiveness':'labels'})\n",
        "            df = df.loc[:,['text','labels']]\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        item = self.df.iloc[index]\n",
        "        text = str(item.text)\n",
        "        tokens = self.tokenizer.encode_plus(text,\n",
        "                                  add_special_tokens= True,\n",
        "                                  return_tensors='pt',\n",
        "                                  truncation=True,\n",
        "#                                   padding='max_length',\n",
        "                                  max_length=self.max_token_len,\n",
        "                                  return_attention_mask = True)\n",
        "        if self.is_test:\n",
        "            return {'input_ids':tokens.input_ids.flatten(),'attention_mask': tokens.attention_mask.flatten()}\n",
        "        else:\n",
        "            # # Convert strings to numerics, follow alphabetical order\n",
        "            attributes = item['labels'].split()\n",
        "            self.label_encoder.fit(self.attributes)\n",
        "            attributes = self.label_encoder.transform(attributes)\n",
        "            attributes = torch.as_tensor(attributes)\n",
        "            #         attributes = torch.FloatTensor(item[self.attributes])\n",
        "            return {'input_ids':tokens.input_ids.flatten(),'attention_mask': tokens.attention_mask.flatten(), 'labels':attributes}\n"
      ],
      "metadata": {
        "id": "M553vuuRbYJp",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.040551Z",
          "iopub.execute_input": "2022-08-03T12:57:27.041180Z",
          "iopub.status.idle": "2022-08-03T12:57:27.064525Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.041137Z",
          "shell.execute_reply": "2022-08-03T12:57:27.063200Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collate (Dynamic Padding)\n",
        "\n",
        "---\n",
        "\n",
        "Dynamically pad tokenized text to match the max length of each batch to reduce computational time"
      ],
      "metadata": {
        "id": "7GuzwhpPeVpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Collate:\n",
        "    def __init__(self, tokenizer, isTrain=True):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.isTrain = isTrain\n",
        "        # self.args = args\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        output = dict()\n",
        "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
        "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
        "        if self.isTrain:\n",
        "            output[\"labels\"] = [sample[\"labels\"] for sample in batch]\n",
        "\n",
        "        # calculate max token length of this batch\n",
        "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
        "\n",
        "        # add padding\n",
        "        if self.tokenizer.padding_side == \"right\":\n",
        "            output[\"input_ids\"] = [s.tolist() + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
        "            output[\"attention_mask\"] = [s.tolist() + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
        "\n",
        "#             output[\"input_ids\"] = [torch.cat(s, torch.FloatTensor((batch_max - len(s)) * [0]), 0) for s in output[\"input_ids\"]]\n",
        "#             output[\"attention_mask\"] = [torch.cat(s, torch.FloatTensor((batch_max - len(s)) * [0]), 0) for s in output[\"attention_mask\"]]\n",
        "        else:\n",
        "            output[\"input_ids\"] = [torch.FloatTensor((batch_max - len(s)) * [self.tokenizer.pad_token_id].tolist()) + s.tolist() for s in output[\"input_ids\"]]\n",
        "            output[\"attention_mask\"] = [torch.FloatTensor((batch_max - len(s)) * [0]) + s.tolist() for s in output[\"attention_mask\"]]\n",
        "            \n",
        "        # convert to tensors\n",
        "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
        "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
        "        if self.isTrain:\n",
        "            output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.long)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.066218Z",
          "iopub.execute_input": "2022-08-03T12:57:27.066663Z",
          "iopub.status.idle": "2022-08-03T12:57:27.083365Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.066621Z",
          "shell.execute_reply": "2022-08-03T12:57:27.082286Z"
        },
        "trusted": true,
        "id": "io80f3u1eVpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder,  attributes=self.attributes, is_train=True, tokenizer=self.tokenizer,text_method=self.text_method)"
      ],
      "metadata": {
        "id": "zCgJTODgeVpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Module\n",
        "\n",
        "---\n",
        "\n",
        "Data preparation by calling dataset and passing it to the dataloader where the data is collated and batched"
      ],
      "metadata": {
        "id": "N3ZT1vpFbYJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _Data_Module(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self, data_path, test_path,attributes,label_encoder,tokenizer, model_name, batch_size: int = 8, max_token_length: int = 512,text_method=0 ):\n",
        "        super().__init__()\n",
        "        self.data_path = data_path\n",
        "        self.test_path = test_path\n",
        "        self.attributes = attributes\n",
        "        self.batch_size = batch_size\n",
        "        self.max_token_length = max_token_length\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = tokenizer #AutoTokenizer.from_pretrained(model_name)\n",
        "        self.label_encoder = label_encoder\n",
        "        self.text_method = text_method\n",
        "\n",
        "    def setup(self, stage = None):\n",
        "        if stage in (None, \"fit\"):\n",
        "            self.train_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder,  attributes=self.attributes, is_train=True, tokenizer=self.tokenizer,text_method=self.text_method)\n",
        "            self.val_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder, attributes=self.attributes, is_train=False,  tokenizer=self.tokenizer,text_method=self.text_method)\n",
        "        if stage == 'predict':\n",
        "            self.test_dataset = _Dataset(self.data_path, self.test_path, label_encoder = self.label_encoder, attributes=self.attributes, is_train=False,is_test=True, tokenizer=self.tokenizer,text_method=self.text_method)\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        collate_fn = Collate(self.tokenizer, \n",
        "                             isTrain=True)\n",
        "\n",
        "        return DataLoader(self.train_dataset, \n",
        "                          batch_size = self.batch_size, \n",
        "                          num_workers=2, \n",
        "                          shuffle=True,\n",
        "                          collate_fn = collate_fn)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        collate_fn = Collate(self.tokenizer, \n",
        "                             isTrain=True)\n",
        "\n",
        "        return DataLoader(self.val_dataset, \n",
        "                          batch_size = self.batch_size, \n",
        "                          num_workers=2, \n",
        "                          shuffle=False,\n",
        "                          collate_fn = collate_fn)\n",
        "\n",
        "    def predict_dataloader(self):\n",
        "        collate_fn = Collate(self.tokenizer, \n",
        "                             isTrain=False)\n",
        "\n",
        "        return DataLoader(self.test_dataset, \n",
        "                          batch_size = self.batch_size, \n",
        "                          num_workers=2, \n",
        "                          shuffle=False,\n",
        "                          collate_fn = collate_fn)\n"
      ],
      "metadata": {
        "id": "_pUquMv6bYJq",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.084542Z",
          "iopub.execute_input": "2022-08-03T12:57:27.085432Z",
          "iopub.status.idle": "2022-08-03T12:57:27.101884Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.085382Z",
          "shell.execute_reply": "2022-08-03T12:57:27.100789Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifier\n",
        "\n",
        "---\n",
        "\n",
        "4 Types of classifier module were created according to different pretrained model:\n",
        "\n",
        "1. Bert Pruned OFA (https://huggingface.co/Intel/bert-large-uncased-sparse-90-unstructured-pruneofa)\n",
        "2. DistilBert (https://huggingface.co/distilbert-base-uncased)\n",
        "3. Deberta-v3-base (https://huggingface.co/microsoft/deberta-v3-base)\n",
        "4. DistilRoberta (https://huggingface.co/distilroberta-base)\n",
        "\n",
        "The classifier module comprises of 5 components:\n",
        "\n",
        "* Computations (init)\n",
        "* Train Loop (training_step)\n",
        "* Validation Loop (validation_step)\n",
        "* Prediction Loop (predict_step)\n",
        "* Optimizers and LR Schedulers (configure_optimizers)\n"
      ],
      "metadata": {
        "id": "hGuHN2DqbYJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. BertOFA Classifier"
      ],
      "metadata": {
        "id": "vP4XgPNueVpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertOFA_Text_Classifier(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, config: dict,data_module):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.data_module=data_module\n",
        "        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = False)\n",
        "        freeze((self.pretrained_model).embeddings)\n",
        "        freeze((self.pretrained_model).encoder.layer[:config['n_freeze']])\n",
        "        print(get_freezed_parameters(self.pretrained_model))\n",
        "        # Adding an additional hidden layer on top of the pretrained model\n",
        "        self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
        "        \n",
        "        # Adding classifier on top of the pretrained model\n",
        "        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
        "        \n",
        "        # Used to initialize the weight of the newly created classifier layer, not sure whether hidden layer need it or not\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        \n",
        "        self.loss_func = nn.CrossEntropyLoss() # do not put SoftMax, just use CrossEntropyLoss\n",
        "        \n",
        "        self.dropout = nn.Dropout(config['p_dropout'])\n",
        "\n",
        "    # For inference        \n",
        "    def forward(self, input_ids, attention_mask, labels = None):\n",
        "        outputs = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "#         pooled_output = torch.mean(outputs.last_hidden_state, 1) \n",
        "#         pooled_output = self.dropout(pooled_output)\n",
        "#         pooled_output = self.dropout(outputs[1])\n",
        "        pooled_output = self.hidden(outputs[1])\n",
        "        pooled_output = F.relu(pooled_output)\n",
        "        pooled_output = self.hidden(pooled_output)\n",
        "        pooled_output = F.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.loss_func(logits,labels)\n",
        "        return loss, logits\n",
        "    \n",
        "    def training_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "            #wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"loss \", loss,on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"loss\":loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n",
        "    \n",
        "    def validation_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "            #wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"val_loss\", loss, on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"val_loss\": loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n",
        "\n",
        "    def predict_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        return logits\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_size = len(self.data_module.train_dataloader())\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
        "        total_steps = train_size/self.config['batch_size']\n",
        "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "        return [optimizer],[scheduler]"
      ],
      "metadata": {
        "id": "P-rc5irkbYJq",
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.103360Z",
          "iopub.execute_input": "2022-08-03T12:57:27.104448Z",
          "iopub.status.idle": "2022-08-03T12:57:27.128381Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.104403Z",
          "shell.execute_reply": "2022-08-03T12:57:27.127152Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. DistilBert classifier"
      ],
      "metadata": {
        "id": "SrU-4OgueVpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistilBert_Text_Classifier(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, config: dict,data_module):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.data_module=data_module\n",
        "        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
        "        freeze((self.pretrained_model).embeddings)\n",
        "        freeze((self.pretrained_model).transformer.layer[:config['n_freeze']])\n",
        "        print(get_freezed_parameters(self.pretrained_model))\n",
        "        # Adding an additional hidden layer on top of the pretrained model\n",
        "        self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size,self.pretrained_model.config.hidden_size)\n",
        "#         self.hidden2 = torch.nn.Linear(self.pretrained_model.config.hidden_size,100)\n",
        "\n",
        "#         self.batchnorm = nn.BatchNorm1d(self.pretrained_model.config.hidden_size)\n",
        "        # Adding classifier on top of the pretrained model\n",
        "        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
        "        \n",
        "        # Used to initialize the weight of the newly created classifier layer, not sure whether hidden layer need it or not\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        \n",
        "        self.loss_func = nn.CrossEntropyLoss() # do not put SoftMax, just use CrossEntropyLoss\n",
        "        \n",
        "        self.dropout = nn.Dropout(config['p_dropout'])\n",
        "\n",
        "    # For inference        \n",
        "    def forward(self, input_ids, attention_mask, labels = None):\n",
        "        output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        logits = self.classifier(output.last_hidden_state)\n",
        "        pooled_output = torch.mean(output.last_hidden_state, 1) \n",
        "        pooled_output = self.hidden(pooled_output)\n",
        "        pooled_output = F.relu(pooled_output)\n",
        "#         pooled_output = self.batchnorm(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "            \n",
        "        # calculate loss\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.loss_func(logits,labels)\n",
        "        return loss, logits\n",
        "    \n",
        "#     def training_step(self, batch, batch_index):\n",
        "#         logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n",
        "# #         print(f\"batch[labels] = {batch['labels']}\")\n",
        "# #         print(f\"{type(batch['labels'])}\")\n",
        "#         class_weights=class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(batch['labels'].tolist()),y=batch['labels'].tolist())\n",
        "#         class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
        "#         loss_func = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to('cuda:0')\n",
        "#         loss = loss_func(logits,batch['labels']).to('cuda:0')\n",
        "        \n",
        "#         f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "#         f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "#         wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n",
        "#         self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "#         self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "#         self.log(\"loss \", loss,on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "#         return {\"loss\":loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n",
        "    \n",
        "#     def validation_step(self, batch, batch_index):\n",
        "#         logits = self(**batch)\n",
        "# #         print(f\"batch[labels] = {batch['labels']}\")\n",
        "#         class_weights=class_weight.compute_class_weight(class_weight='balanced',classes=np.unique(batch['labels'].tolist()),y=batch['labels'].tolist())\n",
        "#         class_weights=torch.tensor(class_weights,dtype=torch.float)\n",
        "#         loss_func = nn.CrossEntropyLoss(weight=class_weights,reduction='mean').to('cuda:0')\n",
        "#         loss = loss_func(logits,batch['labels']).to('cuda:0')\n",
        "        \n",
        "        \n",
        "#         f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "#         f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "#         wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n",
        "#         self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "#         self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "#         self.log(\"val_loss\", loss, on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "#         return {\"val_loss\": loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n",
        "#     \n",
        "#     def predict_step(self, batch, batch_index):\n",
        "#         logits = self(**batch)\n",
        "#         return logits\n",
        "\n",
        "    def training_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "        wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"loss \", loss,on_step=True,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"loss\":loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n",
        "    \n",
        "    def validation_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "        wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=True,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"val_loss\", loss, on_step=True,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"val_loss\": loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n",
        "\n",
        "    def predict_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        return logits\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        train_size = len(self.data_module.train_dataloader())\n",
        "        print(train_size)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
        "        total_steps = train_size/self.config['batch_size']\n",
        "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "        return [optimizer],[scheduler]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.130415Z",
          "iopub.execute_input": "2022-08-03T12:57:27.130913Z",
          "iopub.status.idle": "2022-08-03T12:57:27.156423Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.130869Z",
          "shell.execute_reply": "2022-08-03T12:57:27.155391Z"
        },
        "trusted": true,
        "id": "j8Pu7NuQeVpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Deberta Classifier"
      ],
      "metadata": {
        "id": "0db94Bm3eVpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeBerta_Text_Classifier(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, config: dict,data_module):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.data_module = data_module\n",
        "        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
        "        freeze((self.pretrained_model).embeddings)\n",
        "        freeze((self.pretrained_model).encoder.layer[:config['n_freeze']])\n",
        "        print(get_freezed_parameters(self.pretrained_model))\n",
        "\n",
        "        # Adding an additional hidden layer on top of the pretrained model\n",
        "        self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size,self.pretrained_model.config.hidden_size)\n",
        "        \n",
        "        # Adding classifier on top of the pretrained model\n",
        "        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
        "        \n",
        "        # Used to initialize the weight of the newly created classifier layer, not sure whether hidden layer need it or not\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        \n",
        "        self.loss_func = nn.CrossEntropyLoss() # do not put SoftMax, just use CrossEntropyLoss\n",
        "        \n",
        "        self.dropout = nn.Dropout(config['p_dropout'])\n",
        "\n",
        "    # For inference        \n",
        "    def forward(self, input_ids, attention_mask, labels = None):\n",
        "        # deBERTa layer\n",
        "        output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "\n",
        "        ## instead of only using the first token from the output, we will take the mean of all the tokens in the outputs to learn the representation of the entire sentence\n",
        "        ## pooled_output is the output before going into the final classifier\n",
        "        pooled_output = torch.mean(output.last_hidden_state, 1) \n",
        "        \n",
        "        # final logits\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        pooled_output = self.hidden(pooled_output)\n",
        "        pooled_output = F.relu(pooled_output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        \n",
        "        # calculate loss\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.loss_func(logits,labels)\n",
        "        return loss, logits\n",
        "    \n",
        "    def training_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "            #wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"loss \", loss,on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"loss\":loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n",
        "    \n",
        "    def validation_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "            #wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"val_loss\", loss, on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"val_loss\": loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n",
        "\n",
        "    def predict_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        return logits\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_size = len(self.data_module.train_dataloader())\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
        "        total_steps = train_size/self.config['batch_size']\n",
        "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "        return [optimizer],[scheduler]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.158016Z",
          "iopub.execute_input": "2022-08-03T12:57:27.158594Z",
          "iopub.status.idle": "2022-08-03T12:57:27.180937Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.158551Z",
          "shell.execute_reply": "2022-08-03T12:57:27.179612Z"
        },
        "trusted": true,
        "id": "8BJIcMc9eVpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. DistilRoBerta Classifer"
      ],
      "metadata": {
        "id": "Jaf4z8qxeVpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistilRoBerta_Text_Classifier(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, config: dict,data_module):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.data_module=data_module\n",
        "        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
        "        freeze((self.pretrained_model).embeddings)\n",
        "        freeze((self.pretrained_model).encoder.layer[:config['n_freeze']]) # 5 layer\n",
        "        print(get_freezed_parameters(self.pretrained_model))\n",
        "        # Adding an additional hidden layer on top of the pretrained model\n",
        "        self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size,self.pretrained_model.config.hidden_size)\n",
        "        \n",
        "        # Adding classifier on top of the pretrained model\n",
        "        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
        "        \n",
        "        # Used to initialize the weight of the newly created classifier layer, not sure whether hidden layer need it or not\n",
        "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        \n",
        "        self.loss_func = nn.CrossEntropyLoss() # do not put SoftMax, just use CrossEntropyLoss\n",
        "        \n",
        "        self.dropout = nn.Dropout(config['p_dropout'])\n",
        "\n",
        "    # For inference        \n",
        "    def forward(self, input_ids, attention_mask, labels = None):\n",
        "        output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "        pooled_output = torch.mean(output.last_hidden_state, 1)\n",
        "        logits = self.classifier(pooled_output)        \n",
        "#         pooled_output = self.hidden(pooled_output)\n",
        "#         pooled_output = F.relu(pooled_output)\n",
        "#         pooled_output = self.dropout(pooled_output)\n",
        "        \n",
        "        # calculate loss\n",
        "        loss = 0\n",
        "        if labels is not None:\n",
        "            loss = self.loss_func(logits,labels)\n",
        "        return loss, logits\n",
        "    \n",
        "    def training_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)  # self(**batch) = model(**batch), where **batch = unpack batch\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "            #wandb.log({\"Training Loss\": loss.item(),'Train F1 Score':f1,'Train F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"loss \", loss,on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"loss\":loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"],\"progress_bar\":pbar}\n",
        "    \n",
        "    def validation_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        f1 = f1_score(logits.argmax(dim=1),batch['labels'],num_classes=3,multiclass=True)\n",
        "        f1_weighted = f1_score(logits.softmax(dim=1),batch['labels'],num_classes=3,multiclass=True,average='weighted')\n",
        "            #wandb.log({\"Validation Loss\": loss.item(),'Validation F1 Score':f1,'Validation F1_weighted Score':f1_weighted})\n",
        "        self.log(\"f1\", f1, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"f1_weighted\", f1_weighted, on_step=False,on_epoch=True, prog_bar = True, logger=True)\n",
        "        self.log(\"val_loss\", loss, on_step=False,on_epoch = True, prog_bar = True, logger=True)\n",
        "        return {\"val_loss\": loss}#, \"predictions\":logits, \"labels\": batch[\"labels\"]}\n",
        "\n",
        "    def predict_step(self, batch, batch_index):\n",
        "        loss, logits = self(**batch)\n",
        "        return logits\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        train_size = len(self.data_module.train_dataloader())\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
        "        total_steps = train_size/self.config['batch_size']\n",
        "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "        return [optimizer],[scheduler]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.182834Z",
          "iopub.execute_input": "2022-08-03T12:57:27.183504Z",
          "iopub.status.idle": "2022-08-03T12:57:27.205089Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.183470Z",
          "shell.execute_reply": "2022-08-03T12:57:27.204019Z"
        },
        "trusted": true,
        "id": "AIKv3rkTeVpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "_Yoflk1FeVpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to execute training"
      ],
      "metadata": {
        "id": "KAo-bV5OeVpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config,Text_Classifier,project,samplesize, notes,text_method=0):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n",
        "    le = LabelEncoder()\n",
        "    \n",
        "    data_module = _Data_Module(data_path,\n",
        "                                    test_path,\n",
        "                                    attributes,\n",
        "                                    le,\n",
        "                                    tokenizer,\n",
        "                                    config['model_name'],\n",
        "                                    batch_size=config['batch_size'],\n",
        "                                    text_method=text_method\n",
        "                                   )\n",
        "    data_module.setup()\n",
        "    \n",
        "    # model\n",
        "    model = Text_Classifier(config,data_module)\n",
        "\n",
        "    # trainer and fit\n",
        "    trainer = pl.Trainer(max_epochs=config['n_epochs'],\n",
        "                         accelerator='auto',\n",
        "                         callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\",patience = 3),TQDMProgressBar(refresh_rate=30)],\n",
        "                         default_root_dir=\"./checkpoints\",\n",
        "                         precision = 16,\n",
        "                        ) # automatic mixed precision to reduce memory\n",
        "\n",
        "    # Create a W&B Run\n",
        "    run = wandb.init(name = f\"E{config['n_epochs']}Size{config['batch_size']}Lr{config['lr']}Warm{config['warmup']}Weight{config['weight_decay']}Freeze{config['n_freeze']}Drop{config['p_dropout']}Text{config['text_method']}\" + samplesize,\n",
        "                     notes = str(config) + notes,\n",
        "                     project=project)\n",
        "    trainer.fit(model, data_module)\n",
        "    \n",
        "    run.finish()\n",
        "    \n",
        "    PATH = f\"./{config['name']}_E{config['n_epochs']}Size{config['batch_size']}Lr{config['lr']}Warm{config['warmup']}Weight{config['weight_decay']}Freeze{config['n_freeze']}Drop{config['p_dropout']}Text{config['text_method']}.pth\"\n",
        "    config['PATH'] = PATH\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    \n",
        "    return model,config\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.206683Z",
          "iopub.execute_input": "2022-08-03T12:57:27.207075Z",
          "iopub.status.idle": "2022-08-03T12:57:27.222224Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.207043Z",
          "shell.execute_reply": "2022-08-03T12:57:27.221027Z"
        },
        "trusted": true,
        "id": "8-S-42j7eVpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bertofa_model,bertofa_config = train(config = bertofa_config,\n",
        "#                       Text_Classifier = BertOFA_Text_Classifier,\n",
        "#                       project = 'BertOFA_Text_Classifier',\n",
        "#                       samplesize = ' full',\n",
        "#                       notes = \n",
        "#                       \"\"\"\n",
        "#                         outputs = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask), \n",
        "#                         pooled_output = self.hidden(outputs[1]), \n",
        "#                         pooled_output = F.relu(pooled_output), \n",
        "#                         pooled_output = self.hidden(pooled_output), \n",
        "#                         pooled_output = F.relu(pooled_output), \n",
        "#                         pooled_output = self.dropout(pooled_output), \n",
        "#                         logits = self.classifier(pooled_output)\n",
        "#                         \"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.223897Z",
          "iopub.execute_input": "2022-08-03T12:57:27.225417Z",
          "iopub.status.idle": "2022-08-03T12:57:27.238695Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.225381Z",
          "shell.execute_reply": "2022-08-03T12:57:27.237365Z"
        },
        "trusted": true,
        "id": "M_kvbvl2eVpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilroberta_model,distilroberta_config = train(config = distilroberta_config,\n",
        "                      Text_Classifier = DistilRoBerta_Text_Classifier,\n",
        "                      project = 'DistilRoBerta_Text_Classifier',\n",
        "                      samplesize = ' Full',\n",
        "                      notes = \n",
        "\"\"\"\n",
        "output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask),\n",
        "pooled_output = torch.mean(output.last_hidden_state, 1), \n",
        "logits = self.classifier(pooled_output)  \n",
        "\"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.240917Z",
          "iopub.execute_input": "2022-08-03T12:57:27.241291Z",
          "iopub.status.idle": "2022-08-03T12:57:27.255443Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.241257Z",
          "shell.execute_reply": "2022-08-03T12:57:27.254440Z"
        },
        "trusted": true,
        "id": "ZKjMD842eVpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distilbert_model, distilbert_config  = train(config = distilbert_config,\n",
        "#                                       Text_Classifier = DistilBert_Text_Classifier,\n",
        "#                                       project = 'DistilBert_Text_Classifier',\n",
        "#                                       samplesize = ' 1000',\n",
        "#                                       notes = \n",
        "# \"\"\"\n",
        "# output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask),\n",
        "# pooled_output = torch.mean(output.last_hidden_state, 1),\n",
        "# logits = self.classifier(pooled_output)\n",
        "# \"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.257074Z",
          "iopub.execute_input": "2022-08-03T12:57:27.257409Z",
          "iopub.status.idle": "2022-08-03T12:57:27.266813Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.257380Z",
          "shell.execute_reply": "2022-08-03T12:57:27.265333Z"
        },
        "trusted": true,
        "id": "wfsgqc2peVpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deberta_model,deberta_config = train(config = deberta_config,\n",
        "#                       Text_Classifier = DeBerta_Text_Classifier,\n",
        "#                       project = 'DeBerta_Text_Classifier',\n",
        "#                       samplesize = ' full',\n",
        "#                       notes = \n",
        "# \"\"\"\n",
        "# pooled_output = self.dropout(pooled_output), \n",
        "# pooled_output = self.hidden(pooled_output), \n",
        "# pooled_output = F.relu(pooled_output), \n",
        "# pooled_output = self.dropout(pooled_output), \n",
        "# logits = self.classifier(pooled_output)\n",
        "# \"\"\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.268409Z",
          "iopub.execute_input": "2022-08-03T12:57:27.269372Z",
          "iopub.status.idle": "2022-08-03T12:57:27.277731Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.269323Z",
          "shell.execute_reply": "2022-08-03T12:57:27.276861Z"
        },
        "trusted": true,
        "id": "vaKHdHg9eVpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VALIDATION\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XsFP7u10eVpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(_Text_Classifier,config,data_path,val_path,attributes):    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n",
        "    le = LabelEncoder()\n",
        "    \n",
        "    val_data_module = _Data_Module(data_path,\n",
        "                                    val_path, # using \n",
        "                                    attributes,\n",
        "                                    le,\n",
        "                                    tokenizer,\n",
        "                                    config['model_name'],\n",
        "                                    batch_size=config['batch_size'],\n",
        "                                    text_method = config['text_method']\n",
        "                                   )\n",
        "    val_data_module.setup()\n",
        "\n",
        "    # Initialize Model\n",
        "    model = _Text_Classifier(config,val_data_module)\n",
        "    model.load_state_dict(torch.load(config['PATH']))\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = pl.Trainer(accelerator='auto')\n",
        "\n",
        "#     run = wandb.init(name = f\"Validation\",notes = str(config),project= 'Validation')\n",
        "    \n",
        "    logits = trainer.predict(model, datamodule=val_data_module)\n",
        "\n",
        "#     run.finish()\n",
        "    \n",
        "    pred_list = []\n",
        "    for logit in logits:\n",
        "        pred_list.append(logit)\n",
        "    y_pred = torch.cat(pred_list)\n",
        "\n",
        "    argmax_output = y_pred.argmax(dim=1)\n",
        "    argmax_output = argmax_output.numpy()\n",
        "    \n",
        "    val_df = val_path.copy()\n",
        "    output_df = pd.concat([val_df.reset_index(drop=True), pd.DataFrame(argmax_output,columns=['pred_discourse_effectiveness'])], axis=1)\n",
        "    output_df['pred_discourse_effectiveness'] = output_df['pred_discourse_effectiveness'].map({0:'Adequate',1:'Effective',2:'Ineffective'})\n",
        "\n",
        "    return output_df,y_pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.279483Z",
          "iopub.execute_input": "2022-08-03T12:57:27.280592Z",
          "iopub.status.idle": "2022-08-03T12:57:27.295926Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.280545Z",
          "shell.execute_reply": "2022-08-03T12:57:27.295073Z"
        },
        "trusted": true,
        "id": "Hg6lwNvBeVpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve Validation Data set from train test split"
      ],
      "metadata": {
        "id": "wjKYIYHjeVpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n",
        "\n",
        "# Create validation csv file from traintestsplit\n",
        "df = data_path.copy()\n",
        "y = df['discourse_effectiveness']\n",
        "train_df,val_df = train_test_split(df, test_size=0.2,stratify=y,random_state=91)\n",
        "# y = val_df['discourse_effectiveness']\n",
        "# train_df,val_df = train_test_split(val_df, test_size=0.01,stratify=y,random_state=91)\n",
        "val_path = val_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.297722Z",
          "iopub.execute_input": "2022-08-03T12:57:27.298534Z",
          "iopub.status.idle": "2022-08-03T12:57:27.368927Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.298490Z",
          "shell.execute_reply": "2022-08-03T12:57:27.367973Z"
        },
        "trusted": true,
        "id": "CWVY0EVEeVpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_df['discourse_effectiveness'].value_counts()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.370491Z",
          "iopub.execute_input": "2022-08-03T12:57:27.371137Z",
          "iopub.status.idle": "2022-08-03T12:57:27.385254Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.371091Z",
          "shell.execute_reply": "2022-08-03T12:57:27.383834Z"
        },
        "trusted": true,
        "id": "GY7ueCeQeVpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute Validation"
      ],
      "metadata": {
        "id": "lgfZLRxHeVp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# val_output_df_distilbert, y_pred_distilbert = validate(DistilBert_Text_Classifier,\n",
        "#                                                       distilbert_config,\n",
        "#                                                       data_path,\n",
        "#                                                       val_path,\n",
        "#                                                       attributes)\n",
        "\n",
        "# val_output_df_deberta,y_pred_deberta = validate(DeBerta_Text_Classifier,\n",
        "#                                                   deberta_config,\n",
        "#                                                   data_path,\n",
        "#                                                   val_path,\n",
        "#                                                   attributes)\n",
        "\n",
        "# val_output_df_bertofa, y_pred_bertofa = validate(BertOFA_Text_Classifier,\n",
        "#                                                   bertofa_config,\n",
        "#                                                   data_path,\n",
        "#                                                   val_path,\n",
        "#                                                   attributes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.391023Z",
          "iopub.execute_input": "2022-08-03T12:57:27.391565Z",
          "iopub.status.idle": "2022-08-03T12:57:27.396181Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.391533Z",
          "shell.execute_reply": "2022-08-03T12:57:27.395073Z"
        },
        "trusted": true,
        "id": "jfQ6KBXdeVp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## val_output_df_distilroberta, y_pred_distilroberta = validate(DistilRoBerta_Text_Classifier,\n",
        "##                                                               distilroberta_config,\n",
        "##                                                               data_path,\n",
        "##                                                               val_path,\n",
        "##                                                               attributes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.397915Z",
          "iopub.execute_input": "2022-08-03T12:57:27.398454Z",
          "iopub.status.idle": "2022-08-03T12:57:27.409554Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.398422Z",
          "shell.execute_reply": "2022-08-03T12:57:27.408631Z"
        },
        "trusted": true,
        "id": "bY-1_CUneVp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Validation Results"
      ],
      "metadata": {
        "id": "uzUGpvBDeVp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_distilbert = pd.concat([val_output_df_distilbert.reset_index(drop=True),pd.DataFrame(y_pred_distilbert.tolist(),columns=attributes)],axis=1)\n",
        "# df_deberta = pd.concat([val_output_df_deberta.reset_index(drop=True),pd.DataFrame(y_pred_deberta.tolist(),columns=attributes)],axis=1)\n",
        "# df_bertofa = pd.concat([val_output_df_bertofa.reset_index(drop=True),pd.DataFrame(y_pred_bertofa.tolist(),columns=attributes)],axis=1)\n",
        "\n",
        "# df_distilbert.to_csv('./distilbert_valresult.csv',index=False)\n",
        "# df_deberta.to_csv('./deberta_valresult.csv',index=False)\n",
        "# df_bertofa.to_csv('./bertofa_valresult.csv',index=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.411084Z",
          "iopub.execute_input": "2022-08-03T12:57:27.411822Z",
          "iopub.status.idle": "2022-08-03T12:57:27.426028Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.411774Z",
          "shell.execute_reply": "2022-08-03T12:57:27.424652Z"
        },
        "trusted": true,
        "id": "sLl8c6eseVp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Validation Results"
      ],
      "metadata": {
        "id": "pEb-X2Q2eVp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read data from saved Validation results"
      ],
      "metadata": {
        "id": "ewILdNYreVp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_distilbert=pd.read_csv('../input/validation-result/distilbert_valresult.csv')\n",
        "df_deberta=pd.read_csv('../input/validation-result/deberta_valresult.csv')\n",
        "df_bertofa=pd.read_csv('../input/validation-result/bertofa_valresult.csv')\n",
        "\n",
        "ypred_distilbert = torch.from_numpy(df_distilbert.loc[:,['Adequate','Effective','Ineffective']].values)\n",
        "ypred_deberta = torch.from_numpy(df_deberta.loc[:,['Adequate','Effective','Ineffective']].values)\n",
        "ypred_bertofa = torch.from_numpy(df_bertofa.loc[:,['Adequate','Effective','Ineffective']].values)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:27.427908Z",
          "iopub.execute_input": "2022-08-03T12:57:27.428360Z",
          "iopub.status.idle": "2022-08-03T12:57:29.855699Z",
          "shell.execute_reply.started": "2022-08-03T12:57:27.428317Z",
          "shell.execute_reply": "2022-08-03T12:57:29.854574Z"
        },
        "trusted": true,
        "id": "EXa5JyhyeVp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize weights for ensembling"
      ],
      "metadata": {
        "id": "0ei1-75beVp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight ={\n",
        "        'deberta':0.10,\n",
        "        'distilbert':0.50,\n",
        "        'bertofa':0.40,\n",
        "#         'distilroberta':0.25\n",
        "        }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:29.856935Z",
          "iopub.execute_input": "2022-08-03T12:57:29.857259Z",
          "iopub.status.idle": "2022-08-03T12:57:29.863088Z",
          "shell.execute_reply.started": "2022-08-03T12:57:29.857229Z",
          "shell.execute_reply": "2022-08-03T12:57:29.861662Z"
        },
        "trusted": true,
        "id": "HAslTsIreVp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function for Exploration"
      ],
      "metadata": {
        "id": "C7FfvFSHeVp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert prediction in tensor format to argmax numpy format\n",
        "def pred_to_argmax(y_pred,df,model_name):\n",
        "    argmax_output = y_pred.argmax(dim=1)\n",
        "    argmax_output = argmax_output.numpy()\n",
        "    df = pd.concat([df.reset_index(drop=True),pd.DataFrame(argmax_output,columns=[f'pred_effectiveness_{model_name}'])], axis=1)\n",
        "    df[f'pred_effectiveness_{model_name}'] = df[f'pred_effectiveness_{model_name}'].map({0:'Adequate',1:'Effective',2:'Ineffective'})\n",
        "    return df\n",
        "\n",
        "#Plot confusion matrix\n",
        "def do_conf_matrix(y_true, y_pred, ax, title=None):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=attributes)\n",
        "    cm\n",
        "\n",
        "    sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', ax=ax, cmap='Blues');  \n",
        "    \n",
        "    \n",
        "    # labels, title and ticks\n",
        "    ax.set_xlabel('Predicted labels');\n",
        "    ax.set_ylabel('True labels'); \n",
        "    ax.set_title(f'Confusion Matrix - {title}'); \n",
        "\n",
        "    ax.xaxis.set_ticklabels(attributes)\n",
        "    ax.yaxis.set_ticklabels(attributes);\n",
        "\n",
        "# Calculate metrics\n",
        "def calculate_score(y_pred,df):\n",
        "    cel = nn.CrossEntropyLoss()\n",
        "    f1 = f1_score(y_pred.argmax(dim=1),torch.tensor(df['discourse_effectiveness_numeric'].values),num_classes=3,average=None)\n",
        "    f1_weighted = f1_score(y_pred.softmax(dim=1),torch.tensor(df['discourse_effectiveness_numeric'].values),num_classes=3,multiclass=True,average='weighted')\n",
        "    loss = cel(y_pred,torch.tensor(df['discourse_effectiveness_numeric'].values))\n",
        "    \n",
        "    return f1, f1_weighted, loss\n",
        "\n",
        "# Main validation function\n",
        "def validate_result(y_pred_deberta,y_pred_distilbert,y_pred_bertofa,weight,val_path,preds):\n",
        "\n",
        "#     y_pred_overall = (y_pred_deberta*weight[preds[0]] + y_pred_distilbert*weight[preds[1]] + y_pred_bertofa*weight[preds[2]])/(weight[preds[0]]+weight[preds[1]]+weight[preds[2]])\n",
        "    \n",
        "    weight_deberta = torch.tensor([weight['deberta']])\n",
        "    weight_distilbert = torch.tensor([weight['distilbert']])\n",
        "    weight_bertofa = torch.tensor([weight['bertofa']])\n",
        "\n",
        "    weight_deberta=weight_deberta.repeat(ypred_deberta.shape[0],1)\n",
        "    weight_distilbert=weight_distilbert.repeat(ypred_distilbert.shape[0],1)\n",
        "    weight_bertofa=weight_bertofa.repeat(ypred_bertofa.shape[0],1)\n",
        "\n",
        "#     Method 1\n",
        "#     for idx,ypred in enumerate(ypred_deberta):\n",
        "#         if (ypred[2]>ypred[0]) &(ypred[2]>ypred[1]):\n",
        "#             ratio_bertofa = weight_bertofa[idx]/(weight_distilbert[idx]+weight_deberta[idx])\n",
        "#             ratio_distilbert = weight_distilbert[idx]/(weight_bertofa[idx]+weight_deberta[idx])\n",
        "#             weight_deberta[idx]=0.35\n",
        "#             weight_bertofa[idx]=(1-weight_deberta[idx])*ratio_bertofa\n",
        "#             weight_distilbert[idx]=(1-weight_deberta[idx])*ratio_distilbert\n",
        "\n",
        "# Method 3\n",
        "    for idx,ypred in enumerate(ypred_deberta):\n",
        "        if (ypred[2]>ypred[0]) &(ypred[2]>ypred[1]):# & (ypred_distilbert[idx][2]>ypred_distilbert[idx][0]) &(ypred_distilbert[idx][2]>ypred_distilbert[idx][1]):\n",
        "            ratio_bertofa = weight_bertofa[idx]/(weight_distilbert[idx]+weight_deberta[idx])\n",
        "            ratio_distilbert = weight_distilbert[idx]/(weight_bertofa[idx]+weight_deberta[idx])\n",
        "            weight_deberta[idx]=0.6\n",
        "            weight_bertofa[idx]=0.02\n",
        "            weight_distilbert[idx]=0.38 \n",
        "\n",
        "    \n",
        "    numerator = torch.mul(ypred_deberta,weight_deberta) + torch.mul(ypred_distilbert,weight_bertofa) + torch.mul(ypred_bertofa,weight_distilbert)\n",
        "    denominator = torch.add(torch.add(weight_deberta,weight_bertofa),weight_distilbert)\n",
        "    y_pred_overall = torch.div(numerator,denominator)\n",
        "    \n",
        "#         # Method 2\n",
        "#     for idx,ypred in enumerate(y_pred_overall):\n",
        "#         if ypred[]\n",
        "#         if (ypred[2]>ypred[0]) &(ypred[2]>ypred[1])& (ypred_distilbert[idx][2]<ypred_distilbert[idx][0])& (ypred_distilbert[idx][2]<ypred_distilbert[idx][1]):\n",
        "#             ratio_bertofa = weight_bertofa[idx]/(weight_distilbert[idx]+weight_deberta[idx])\n",
        "#             ratio_distilbert = weight_distilbert[idx]/(weight_bertofa[idx]+weight_deberta[idx])\n",
        "#             weight_deberta[idx]=0.35\n",
        "#             weight_bertofa[idx]=(1-weight_deberta[idx])*ratio_bertofa\n",
        "#             weight_distilbert[idx]=(1-weight_deberta[idx])*ratio_distilbert  \n",
        "    \n",
        "    df = val_path.copy()\n",
        "    df['discourse_effectiveness_numeric'] = df['discourse_effectiveness'].map({'Adequate':0,'Effective':1,'Ineffective':2})\n",
        "    y_true = df['discourse_effectiveness'].values\n",
        "    \n",
        "\n",
        "    \n",
        "    fig, axs = plt.subplots(2,2,figsize=(20, 10))\n",
        "    \n",
        "    df = pred_to_argmax(y_pred_deberta,df,preds[0])    \n",
        "    df = pred_to_argmax(y_pred_distilbert,df,preds[1])\n",
        "    df = pred_to_argmax(y_pred_bertofa,df,preds[2])\n",
        "    df = pred_to_argmax(y_pred_overall,df,preds[3])\n",
        "    \n",
        "    print(df.info())\n",
        "    do_conf_matrix(y_true, df[f'pred_effectiveness_deberta'].values, ax=axs[0,0],title = preds[0])\n",
        "    do_conf_matrix(y_true, df[f'pred_effectiveness_distilbert'].values, ax=axs[1,0],title = preds[1])\n",
        "    do_conf_matrix(y_true, df[f'pred_effectiveness_bertofa'].values, ax=axs[0,1],title = preds[2])\n",
        "    do_conf_matrix(y_true, df[f'pred_effectiveness_OVERALL'].values, ax=axs[1,1],title = preds[3])\n",
        "\n",
        "    output1 = calculate_score(y_pred_deberta,df)\n",
        "    output2 = calculate_score(y_pred_distilbert,df)\n",
        "    output3 = calculate_score(y_pred_bertofa,df)\n",
        "    output4 = calculate_score(y_pred_overall,df)\n",
        "\n",
        "    return output1, output2, output3, output4\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:29.865247Z",
          "iopub.execute_input": "2022-08-03T12:57:29.866201Z",
          "iopub.status.idle": "2022-08-03T12:57:29.893339Z",
          "shell.execute_reply.started": "2022-08-03T12:57:29.866154Z",
          "shell.execute_reply": "2022-08-03T12:57:29.891881Z"
        },
        "trusted": true,
        "id": "hfaXJJNHeVp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "preds = ['deberta','distilbert','bertofa','OVERALL']\n",
        "\n",
        "output1, output2, output3, output4 = validate_result(ypred_deberta,ypred_distilbert,ypred_bertofa,weight,val_path, preds)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:29.895056Z",
          "iopub.execute_input": "2022-08-03T12:57:29.895854Z",
          "iopub.status.idle": "2022-08-03T12:57:31.874857Z",
          "shell.execute_reply.started": "2022-08-03T12:57:29.895806Z",
          "shell.execute_reply": "2022-08-03T12:57:31.873826Z"
        },
        "trusted": true,
        "id": "m_ZQsoWAeVp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read F1 Score for each class"
      ],
      "metadata": {
        "id": "9QbFzO9beVp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "f1_score_df = pd.DataFrame({'deberta':output1[0].tolist(),\n",
        "                         'distilbert':output2[0].tolist(),\n",
        "                         'bertofa':output3[0].tolist(),\n",
        "                         'overall':output4[0].tolist(),\n",
        "                            })\n",
        "f1_score_df['label']=attributes\n",
        "f1_score_df.set_index(['label'])\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:31.877882Z",
          "iopub.execute_input": "2022-08-03T12:57:31.878292Z",
          "iopub.status.idle": "2022-08-03T12:57:31.903789Z",
          "shell.execute_reply.started": "2022-08-03T12:57:31.878259Z",
          "shell.execute_reply": "2022-08-03T12:57:31.902952Z"
        },
        "trusted": true,
        "id": "Py9a-JlHeVp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read weighted average f1 score from various models, including ensembled model"
      ],
      "metadata": {
        "id": "SBlZXNkzeVp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_weight_score = []\n",
        "f1_weight_score.append(output1[1].item())\n",
        "f1_weight_score.append(output2[1].item())\n",
        "f1_weight_score.append(output3[1].item())\n",
        "f1_weight_score.append(output4[1].item())\n",
        "f1_weight_score\n",
        "\n",
        "logloss_score = []\n",
        "logloss_score.append(output1[2].item())\n",
        "logloss_score.append(output2[2].item())\n",
        "logloss_score.append(output3[2].item())\n",
        "logloss_score.append(output4[2].item())\n",
        "logloss_score\n",
        "\n",
        "score_df = pd.DataFrame({'f1_weighted_score':f1_weight_score})\n",
        "score_df['log loss'] = logloss_score\n",
        "score_df['model']=preds\n",
        "score_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:31.907806Z",
          "iopub.execute_input": "2022-08-03T12:57:31.908510Z",
          "iopub.status.idle": "2022-08-03T12:57:31.924696Z",
          "shell.execute_reply.started": "2022-08-03T12:57:31.908474Z",
          "shell.execute_reply": "2022-08-03T12:57:31.923324Z"
        },
        "trusted": true,
        "id": "XUkyBFuFeVp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # y_pred_overall = (y_pred_deberta*weight['deberta'] + y_pred_distilroberta*weight['distilbert'] + y_pred_bertofa*weight['bertofa'] + y_pred_distilbert*weight['distilroberta'])/4\n",
        "# y_pred_overall = (y_pred_deberta*weight['deberta'] + y_pred_distilbert*weight['distilbert'] + y_pred_bertofa*weight['bertofa'])/(weight['deberta']+weight['distilbert']+weight['bertofa'])\n",
        "\n",
        "\n",
        "# argmax_output = y_pred_overall.argmax(dim=1)\n",
        "# argmax_output = argmax_output.numpy()\n",
        "# output_overall_df = pd.concat([val_df.reset_index(drop=True), pd.DataFrame(argmax_output,columns=['pred_discourse_effectiveness'])], axis=1)\n",
        "# output_overall_df['pred_discourse_effectiveness'] = output_overall_df['pred_discourse_effectiveness'].map({0:'Adequate',1:'Effective',2:'Ineffective'})\n",
        "# #Plot Confusion Matrix\n",
        "# y_true = output_df['discourse_effectiveness'].values\n",
        "# y_pred = output_df['pred_discourse_effectiveness'].values\n",
        "# ax= plt.subplot()\n",
        "# do_conf_matrix(y_true, y_pred, ax=ax)\n",
        "# output_overall_df['discourse_effectiveness_numeric'] = output_overall_df['discourse_effectiveness'].map({'Adequate':0,'Effective':1,'Ineffective':2})\n",
        "# output_overall_df.info()\n",
        "# f1 = f1_score(y_pred_overall.argmax(dim=1),torch.tensor(output_overall_df['discourse_effectiveness_numeric'].values),average=None)\n",
        "# f1\n",
        "# f1_weighted = f1_score(y_pred_overall.softmax(dim=1),torch.tensor(output_overall_df['discourse_effectiveness_numeric'].values),num_classes=3,multiclass=True,average='weighted')\n",
        "# f1_weighted\n",
        "# cel = nn.CrossEntropyLoss()\n",
        "# loss = cel(y_pred_overall,torch.tensor(output_overall_df['discourse_effectiveness_numeric'].values))\n",
        "# loss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:31.926586Z",
          "iopub.execute_input": "2022-08-03T12:57:31.927565Z",
          "iopub.status.idle": "2022-08-03T12:57:31.932939Z",
          "shell.execute_reply.started": "2022-08-03T12:57:31.927528Z",
          "shell.execute_reply": "2022-08-03T12:57:31.931721Z"
        },
        "trusted": true,
        "id": "PeI5-TZgeVp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read text with wrong classification"
      ],
      "metadata": {
        "id": "wcvaLpyoeVp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_overall = (ypred_deberta*weight['deberta'] + ypred_distilbert*weight['distilbert'] + ypred_bertofa*weight['bertofa'])/(weight['deberta']+weight['distilbert']+weight['bertofa'])\n",
        "argmax_output = y_pred_overall.argmax(dim=1)\n",
        "argmax_output = argmax_output.numpy()\n",
        "output_overall_df = pd.concat([val_df.reset_index(drop=True), pd.DataFrame(argmax_output,columns=['pred_discourse_effectiveness'])], axis=1)\n",
        "output_overall_df['pred_discourse_effectiveness'] = output_overall_df['pred_discourse_effectiveness'].map({0:'Adequate',1:'Effective',2:'Ineffective'})\n",
        "\n",
        "output_overall_df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T12:57:31.936143Z",
          "iopub.execute_input": "2022-08-03T12:57:31.937113Z",
          "iopub.status.idle": "2022-08-03T12:57:31.966187Z",
          "shell.execute_reply.started": "2022-08-03T12:57:31.937075Z",
          "shell.execute_reply": "2022-08-03T12:57:31.965295Z"
        },
        "trusted": true,
        "id": "nZvb4Y_NeVp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_pred_df = output_overall_df[output_overall_df['pred_discourse_effectiveness']!=output_overall_df['discourse_effectiveness']]\n",
        "ineffective_bad_pred_df = bad_pred_df.loc[(bad_pred_df['pred_discourse_effectiveness']!=bad_pred_df['discourse_effectiveness'])& (bad_pred_df['discourse_effectiveness']=='Ineffective')]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T13:04:35.292119Z",
          "iopub.execute_input": "2022-08-03T13:04:35.292517Z",
          "iopub.status.idle": "2022-08-03T13:04:35.303554Z",
          "shell.execute_reply.started": "2022-08-03T13:04:35.292485Z",
          "shell.execute_reply": "2022-08-03T13:04:35.302414Z"
        },
        "trusted": true,
        "id": "XZht0GX8eVp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(ids):\n",
        "    with open(f'../input/feedback-prize-effectiveness/train/{ids}.txt', 'r') as file: data = file.read()\n",
        "    return data\n",
        "\n",
        "def display_sample(essay_id,bad_df,train_df):\n",
        "    char_pos = 0\n",
        "    ex = [{\"text\": '',\"ents\": [],\"title\":\"\"}]\n",
        "    ex2 = [{\"text\": '',\"ents\": [],\"title\":\"\"}]\n",
        "    text = ''\n",
        "    for idx in range(train_df.loc[(train_df['essay_id']==essay_id)].shape[0]):\n",
        "        \n",
        "        discourse_text = train_df.loc[(df.essay_id == essay_id),'discourse_text'].values[idx]\n",
        "        discourse_text = resolve_encodings_and_normalize(discourse_text)\n",
        "        begin = char_pos\n",
        "        end = begin + len(discourse_text)\n",
        "        discoursetype = train_df.loc[(train_df.essay_id == essay_id),'discourse_type'].values[idx]\n",
        "        discourse_id = train_df.loc[(train_df.essay_id == essay_id),'discourse_id'].values[idx]\n",
        "        \n",
        "        if bad_df[bad_df.discourse_id == discourse_id].shape[0] != 0:\n",
        "            label_bad = bad_df.loc[(bad_df.discourse_id == discourse_id),'pred_discourse_effectiveness'].values[0]\n",
        "            label_good = bad_df.loc[(bad_df.discourse_id == discourse_id),'discourse_effectiveness'].values[0]\n",
        "            ex[0]['ents'].append({\"start\":begin,\n",
        "                  \"end\":end,\n",
        "                  \"label\":label_bad + '/' + label_good+ ' (Predict/True)' + ' - ' + discoursetype\n",
        "                    })\n",
        "\n",
        "            ex2[0]['ents'].append({\"start\":begin,\n",
        "                  \"end\":end,\n",
        "                  \"label\":label_good + ' - ' + discoursetype + ' (True)'\n",
        "                    })\n",
        "        char_pos = end\n",
        "        text += discourse_text\n",
        "    ex[0]['text']=text\n",
        "    ex2[0]['text']=text\n",
        "    ex[0]['title']=f\"Essay ID: {essay_id}\"\n",
        "    displacy.render(ex, style=\"ent\", manual=True,jupyter=True,options={\"distance\":100})\n",
        "    print()\n",
        "#     displacy.render(ex2, style=\"ent\", manual=True,jupyter=True,options={\"distance\":100})\n",
        "    return\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T13:04:59.977027Z",
          "iopub.execute_input": "2022-08-03T13:04:59.978118Z",
          "iopub.status.idle": "2022-08-03T13:04:59.993103Z",
          "shell.execute_reply.started": "2022-08-03T13:04:59.978071Z",
          "shell.execute_reply": "2022-08-03T13:04:59.991805Z"
        },
        "trusted": true,
        "id": "gF1LG0voeVp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_id = ineffective_bad_pred_df.sample(1)['essay_id'].values[0]\n",
        "display_sample(sample_id,bad_pred_df,data_path.sort_index())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T13:07:10.255337Z",
          "iopub.execute_input": "2022-08-03T13:07:10.255837Z",
          "iopub.status.idle": "2022-08-03T13:07:10.332695Z",
          "shell.execute_reply.started": "2022-08-03T13:07:10.255800Z",
          "shell.execute_reply": "2022-08-03T13:07:10.331173Z"
        },
        "trusted": true,
        "id": "d_NhAguZeVp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PREDICTION AND KAGGLE SUBMISSION\n",
        "---\n",
        "\n",
        "https://www.kaggle.com/competitions/feedback-prize-effectiveness\n"
      ],
      "metadata": {
        "id": "jnvJg9SLty00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function to execute prediction on test dataset"
      ],
      "metadata": {
        "id": "sZzcMOiLeVp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(_Text_Classifier,config):\n",
        "    \n",
        "    data_path = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/train.csv')\n",
        "    test_path = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')\n",
        "    attributes = [\"Adequate\" ,\"Effective\",\"Ineffective\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config['model_name'], use_fast=True)\n",
        "    le = LabelEncoder()\n",
        "    \n",
        "    # Initialize data module\n",
        "    test_data_module = _Data_Module(data_path,\n",
        "                                    test_path,\n",
        "                                    attributes,\n",
        "                                    le,\n",
        "                                    tokenizer,\n",
        "                                    config['model_name'],\n",
        "                                    batch_size=config['batch_size'],\n",
        "                                    text_method = config['text_method']\n",
        "                                   )\n",
        "    test_data_module.setup()\n",
        "\n",
        "    # Initialize Model\n",
        "    model = _Text_Classifier(config,test_data_module)\n",
        "    model.load_state_dict(torch.load(config['PATH']))\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = pl.Trainer(accelerator='auto')\n",
        "\n",
        "    # Run predictions\n",
        "    def predict_text_classification(model, dm):\n",
        "        predictions = trainer.predict(model, datamodule=dm)\n",
        "        return predictions\n",
        "    predictions = predict_text_classification(model, test_data_module)\n",
        "\n",
        "    # Pass logit into a softmax\n",
        "    pred_list = []\n",
        "    for logits in predictions:\n",
        "        pred_list.append(logits)\n",
        "    y_pred = torch.cat(pred_list)\n",
        "    y_pred.shape\n",
        "\n",
        "    softmax_outputs = softmax(y_pred, axis=1)\n",
        "    test_df = test_path.copy()\n",
        "    output_df = pd.concat([test_df[['discourse_id']].reset_index(drop=True), pd.DataFrame(softmax_outputs.numpy(), columns=attributes)], axis=1)\n",
        "#     output_df = pd.concat([test_df[['discourse_id']].reset_index(drop=True), pd.DataFrame(y_pred.numpy(), columns=attributes)], axis=1)\n",
        "    new_cols = [\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"]\n",
        "    output_df = output_df[new_cols]\n",
        "\n",
        "    return output_df, y_pred"
      ],
      "metadata": {
        "id": "RgBhridfs8uF",
        "execution": {
          "iopub.status.busy": "2022-08-03T04:05:44.306196Z",
          "iopub.execute_input": "2022-08-03T04:05:44.306511Z",
          "iopub.status.idle": "2022-08-03T04:05:44.320763Z",
          "shell.execute_reply.started": "2022-08-03T04:05:44.306478Z",
          "shell.execute_reply": "2022-08-03T04:05:44.319680Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict using multiple models"
      ],
      "metadata": {
        "id": "PWTL8MQ3eVp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_DeBerta, y_pred_deberta = predict(DeBerta_Text_Classifier,deberta_config)\n",
        "output_DistilBert, y_pred_distilbert = predict(DistilBert_Text_Classifier,distilbert_config)\n",
        "output_BertOFA, y_pred_bertofa = predict(BertOFA_Text_Classifier,bertofa_config)\n",
        "# output_distilRoBerta = predict(DistilRoBerta_Text_Classifier,distilroberta_config)\n",
        "\n"
      ],
      "metadata": {
        "id": "D871lnt73fYK",
        "execution": {
          "iopub.status.busy": "2022-08-03T04:05:44.322607Z",
          "iopub.execute_input": "2022-08-03T04:05:44.323097Z",
          "iopub.status.idle": "2022-08-03T04:07:17.567180Z",
          "shell.execute_reply.started": "2022-08-03T04:05:44.323056Z",
          "shell.execute_reply": "2022-08-03T04:07:17.564718Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# output_df = output_DeBerta.copy()\n",
        "\n",
        "# for attribute in attributes:\n",
        "#     output_df[attribute] =  (output_DeBerta[attribute]*weight['deberta'] + \n",
        "#                             output_DistilBert[attribute]*weight['distilbert'] + \n",
        "#                             output_BertOFA[attribute]*weight['bertofa'])/(weight['deberta']+weight['distilbert']+weight['bertofa'])\n",
        "# for atttribute in attributes:\n",
        "#     output_df[attribute] =  (output_DeBerta[attribute]*weight['deberta'] + \n",
        "#                             output_DistilBert[attribute]*weight['distilbert'] + \n",
        "#                             output_BertOFA[attribute]*weight['bertofa'] +\n",
        "#                             output_distilRoBerta[attribute]*weight['distilroberta'])/4\n",
        "\n",
        "# output_df"
      ],
      "metadata": {
        "id": "IubxfQVeR8OD",
        "execution": {
          "iopub.status.busy": "2022-08-03T04:07:17.569883Z",
          "iopub.execute_input": "2022-08-03T04:07:17.571288Z",
          "iopub.status.idle": "2022-08-03T04:07:17.589588Z",
          "shell.execute_reply.started": "2022-08-03T04:07:17.571214Z",
          "shell.execute_reply": "2022-08-03T04:07:17.587908Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble predicted values by various model and pass it to softmax to normalize data to be between 0 to 1"
      ],
      "metadata": {
        "id": "t50XsmoVeVp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_deberta = torch.tensor([weight['deberta']])\n",
        "weight_distilbert = torch.tensor([weight['distilbert']])\n",
        "weight_bertofa = torch.tensor([weight['bertofa']])\n",
        "\n",
        "weight_deberta=weight_deberta.repeat(y_pred_deberta.shape[0],1)\n",
        "weight_distilbert=weight_distilbert.repeat(y_pred_distilbert.shape[0],1)\n",
        "weight_bertofa=weight_bertofa.repeat(y_pred_bertofa.shape[0],1)\n",
        "        \n",
        "for idx,ypred in enumerate(y_pred_deberta):\n",
        "    if (ypred[2]>ypred[0]) &(ypred[2]>ypred[1]):\n",
        "        ratio_bertofa = weight_bertofa[idx]/(weight_distilbert[idx]+weight_deberta[idx])\n",
        "        ratio_distilbert = weight_distilbert[idx]/(weight_bertofa[idx]+weight_deberta[idx])\n",
        "        weight_deberta[idx]=0.6\n",
        "        weight_bertofa[idx]=0.02\n",
        "        weight_distilbert[idx]=0.38 \n",
        "        \n",
        "numerator = torch.mul(y_pred_deberta,weight_deberta) + torch.mul(y_pred_distilbert,weight_bertofa) + torch.mul(y_pred_bertofa,weight_distilbert)\n",
        "denominator = torch.add(torch.add(weight_deberta,weight_bertofa),weight_distilbert)\n",
        "y_pred_overall = torch.div(numerator,denominator)\n",
        "\n",
        "softmax_outputs = softmax(y_pred_overall, axis=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T04:09:08.946409Z",
          "iopub.execute_input": "2022-08-03T04:09:08.947017Z",
          "iopub.status.idle": "2022-08-03T04:09:08.965510Z",
          "shell.execute_reply.started": "2022-08-03T04:09:08.946972Z",
          "shell.execute_reply": "2022-08-03T04:09:08.964545Z"
        },
        "trusted": true,
        "id": "iCvOU_ZAeVqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe for submission\n",
        "output_df = pd.concat([test_path[['discourse_id']].reset_index(drop=True), pd.DataFrame(softmax_outputs.numpy(), columns=attributes)], axis=1)\n",
        "#     output_df = pd.concat([test_df[['discourse_id']].reset_index(drop=True), pd.DataFrame(y_pred.numpy(), columns=attributes)], axis=1)\n",
        "\n",
        "# Re-arrange columns\n",
        "new_cols = [\"discourse_id\",\"Ineffective\",\"Adequate\",\"Effective\"]\n",
        "output_df = output_df[new_cols]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-08-03T04:09:19.157807Z",
          "iopub.execute_input": "2022-08-03T04:09:19.158521Z",
          "iopub.status.idle": "2022-08-03T04:09:19.168002Z",
          "shell.execute_reply.started": "2022-08-03T04:09:19.158483Z",
          "shell.execute_reply": "2022-08-03T04:09:19.166831Z"
        },
        "trusted": true,
        "id": "6TOg6lyoeVqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_df.to_csv('submission.csv', index=False)\n",
        "pd.read_csv('submission.csv')"
      ],
      "metadata": {
        "id": "NylMBGU1P-z6",
        "execution": {
          "iopub.status.busy": "2022-08-03T04:09:23.166366Z",
          "iopub.execute_input": "2022-08-03T04:09:23.166736Z",
          "iopub.status.idle": "2022-08-03T04:09:23.187501Z",
          "shell.execute_reply.started": "2022-08-03T04:09:23.166702Z",
          "shell.execute_reply": "2022-08-03T04:09:23.186294Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zbpe-WPReVqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}